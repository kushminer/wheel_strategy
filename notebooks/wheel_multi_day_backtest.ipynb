{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multi-Day Wheel Strategy Backtest\n",
    "\n",
    "This notebook processes multiple entry dates for the wheel strategy backtest.\n",
    "\n",
    "**Features:**\n",
    "- Caches API responses to minimize requests\n",
    "- Tracks active positions\n",
    "- Prevents same-day duplicate entries (re-entry on different dates IS allowed)\n",
    "- Easy to expand by adding more dates to `entry_dates` list\n",
    "\n",
    "**Validation:**\n",
    "Start with `entry_dates = ['2023-06-06']` to compare with single-day notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"/Users/samuelminer/Projects/nissan_options/wheel_strategy/.env\")\n",
    "load_dotenv(env_path, override=True)\n",
    "assert os.getenv(\"DATABENTO_API_KEY\"), \"DATABENTO_API_KEY not found\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import databento as db\n",
    "import pandas_market_calendars as mcal\n",
    "from py_vollib.black_scholes.implied_volatility import implied_volatility\n",
    "from py_vollib.black_scholes.greeks.analytical import delta\n",
    "\n",
    "# Initialize clients\n",
    "client = db.Historical()\n",
    "nyse = mcal.get_calendar(\"NYSE\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": "CONFIG = {\n    # Ticker and dates\n    'ticker': 'TSLA',\n    'entry_dates': ['2023-06-06'],  # Start with one date to validate\n\n    # OR use date range (will be converted to trading days automatically)\n    # 'start_date': '2023-06-01',\n    # 'end_date': '2023-06-30',\n\n    # Timezone\n    'timezone': 'America/New_York',\n\n    # Cache directory\n    'cache_dir': '../cache/',\n\n    # Technical filter settings\n    'technical_filter_enabled': True,  # Set to False to disable\n    'bb_window': 20,                   # Bollinger Band lookback period\n    'bb_std': 2.0,                     # Bollinger Band standard deviations\n    'require_sma_entry': True,         # Entry when close <= SMA20\n    'require_bb_entry': False,         # Entry when close <= lower BB (more restrictive)\n\n    # Option filters\n    'min_dte': 30,\n    'max_dte': 45,\n    'min_delta': 0.25,\n    'max_delta': 0.35,\n    'option_type': 'P',\n\n    # Exit strategy\n    'profit_target_pct': 0.50,\n    'exit_dte': 21,\n\n    # Risk-free rate for IV/delta calculation\n    'risk_free_rate': 0.04,\n}\n\n# Create cache directory if needed\nos.makedirs(CONFIG['cache_dir'], exist_ok=True)\n\n# Convert date range to list of trading days if specified\nif 'start_date' in CONFIG and 'end_date' in CONFIG:\n    start = pd.Timestamp(CONFIG['start_date'])\n    end = pd.Timestamp(CONFIG['end_date'])\n\n    # Get trading days from NYSE calendar\n    trading_days = nyse.valid_days(start_date=start, end_date=end)\n    CONFIG['entry_dates'] = [d.strftime('%Y-%m-%d') for d in trading_days]\n\n    print(f\"Date range: {CONFIG['start_date']} to {CONFIG['end_date']}\")\n    print(f\"Generated {len(CONFIG['entry_dates'])} trading days\")\n    print(f\"First 5: {CONFIG['entry_dates'][:5]}\")\n    print(f\"Last 5: {CONFIG['entry_dates'][-5:]}\")\nelse:\n    print(f\"Ticker: {CONFIG['ticker']}\")\n    print(f\"Entry dates: {CONFIG['entry_dates']}\")\n\nprint(f\"Cache dir: {CONFIG['cache_dir']}\")\nprint(f\"Technical filter: {'ENABLED' if CONFIG['technical_filter_enabled'] else 'DISABLED'}\")\nif CONFIG['technical_filter_enabled']:\n    print(f\"  SMA entry (close <= SMA{CONFIG['bb_window']}): {CONFIG['require_sma_entry']}\")\n    print(f\"  BB entry (close <= lower BB): {CONFIG['require_bb_entry']}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Caching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(name):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(CONFIG['cache_dir'], f\"{name}.parquet\")\n",
    "\n",
    "def load_from_cache(name):\n",
    "    \"\"\"Load DataFrame from cache if it exists\"\"\"\n",
    "    path = get_cache_path(name)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  [CACHE HIT] Loading {name}\")\n",
    "        return pd.read_parquet(path)\n",
    "    return None\n",
    "\n",
    "def save_to_cache(df, name):\n",
    "    \"\"\"Save DataFrame to cache\"\"\"\n",
    "    path = get_cache_path(name)\n",
    "    df.to_parquet(path)\n",
    "    print(f\"  [CACHE SAVE] Saved {name}\")\n",
    "\n",
    "print(\"Caching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_option_symbols(df):\n",
    "    \"\"\"Parse OPRA symbols into components\"\"\"\n",
    "    sym = df[\"symbol\"]\n",
    "    \n",
    "    # Split ROOT and OPRA code\n",
    "    root_and_code = sym.str.split(expand=True)\n",
    "    df[\"root\"] = root_and_code[0]\n",
    "    code = root_and_code[1]\n",
    "    \n",
    "    # Expiration: YYMMDD\n",
    "    df[\"expiration\"] = pd.to_datetime(code.str[:6], format=\"%y%m%d\")\n",
    "    \n",
    "    # Call/Put flag\n",
    "    df[\"call_put\"] = code.str[6]\n",
    "    \n",
    "    # Strike: in 1/1000 dollars\n",
    "    strike_int = code.str[7:].astype(\"int32\")\n",
    "    df[\"strike\"] = strike_int / 1000.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_trading_dte(df, tz=\"America/New_York\"):\n",
    "    \"\"\"Add trading-days-to-expiration using NYSE calendar\"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    # Event dates from ts_event column\n",
    "    event_dt = pd.to_datetime(out[\"ts_event\"]).dt.tz_convert(tz).dt.normalize()\n",
    "    event_days = pd.to_datetime(event_dt.dt.date)  # tz-naive\n",
    "    \n",
    "    # Expiration dates\n",
    "    exp_dt = pd.to_datetime(out[\"expiration\"])\n",
    "    exp_days = pd.to_datetime(exp_dt.dt.date)  # tz-naive\n",
    "    \n",
    "    # Build trading calendar\n",
    "    start_date = event_days.min().date()\n",
    "    end_date = exp_days.max().date()\n",
    "    \n",
    "    schedule = nyse.valid_days(start_date=start_date, end_date=end_date)\n",
    "    schedule = pd.to_datetime(schedule).normalize().tz_localize(None)\n",
    "    \n",
    "    cal_index = pd.Series(np.arange(len(schedule), dtype=np.int32), index=schedule)\n",
    "    \n",
    "    event_idx = cal_index.reindex(event_days).to_numpy()\n",
    "    exp_idx = cal_index.reindex(exp_days).to_numpy()\n",
    "    \n",
    "    out[\"dte\"] = (exp_idx - event_idx - 1).astype(np.int16)\n",
    "    return out\n",
    "\n",
    "\n",
    "def calculate_21dte_dates(expirations):\n",
    "    \"\"\"Calculate dates 21 trading days before expiration\"\"\"\n",
    "    min_exp = expirations.min()\n",
    "    max_exp = expirations.max()\n",
    "    \n",
    "    start_date = min_exp - pd.Timedelta(days=60)\n",
    "    end_date = max_exp\n",
    "    \n",
    "    schedule = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "    trading_days = schedule.index\n",
    "    \n",
    "    results = []\n",
    "    for exp in expirations:\n",
    "        exp_dt = pd.Timestamp(exp).normalize()\n",
    "        valid_days = trading_days[trading_days <= exp_dt]\n",
    "        \n",
    "        if len(valid_days) >= 21:\n",
    "            target_date = valid_days[-21]\n",
    "        else:\n",
    "            target_date = valid_days[0] if len(valid_days) > 0 else exp_dt\n",
    "        \n",
    "        results.append(target_date)\n",
    "    \n",
    "    return pd.Series(results, index=expirations.index)\n",
    "\n",
    "\n",
    "def compute_iv(row, r):\n",
    "    \"\"\"Compute implied volatility\"\"\"\n",
    "    price = row[\"mid\"]\n",
    "    S = row[\"underlying_last\"]\n",
    "    K = row[\"strike\"]\n",
    "    t = row[\"dte\"] / 365.0\n",
    "    flag = \"p\" if row[\"call_put\"] == \"P\" else \"c\"\n",
    "\n",
    "    if not (np.isfinite(price) and np.isfinite(S) and np.isfinite(K) and t > 0):\n",
    "        return np.nan\n",
    "    if price <= 0 or S <= 0 or K <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return implied_volatility(price, S, K, t, r, flag)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_delta(row, r):\n",
    "    \"\"\"Compute delta using IV\"\"\"\n",
    "    sigma = row[\"iv\"]\n",
    "    if not np.isfinite(sigma):\n",
    "        return np.nan\n",
    "\n",
    "    S = row[\"underlying_last\"]\n",
    "    K = row[\"strike\"]\n",
    "    t = row[\"dte\"] / 365.0\n",
    "    flag = \"p\" if row[\"call_put\"] == \"P\" else \"c\"\n",
    "\n",
    "    return delta(flag, S, K, t, r, sigma)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Data Fetch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-functions",
   "metadata": {},
   "outputs": [],
   "source": "def fetch_options_snapshot(ticker, date):\n    \"\"\"Fetch option chain at 15:45 ET, with caching\"\"\"\n    cache_name = f\"options_{ticker}_{date}\"\n    \n    # Try cache first\n    cached = load_from_cache(cache_name)\n    if cached is not None:\n        return cached\n    \n    # Fetch from API\n    print(f\"  [API] Fetching options for {ticker} on {date}...\")\n    \n    tz = CONFIG['timezone']\n    start_time = pd.Timestamp(f\"{date} 15:45\", tz=tz)\n    end_time = start_time + pd.Timedelta(minutes=1)\n    \n    data = client.timeseries.get_range(\n        dataset='OPRA.PILLAR',\n        schema='cmbp-1',\n        symbols=f'{ticker}.OPT',\n        stype_in='parent',\n        start=start_time,\n        end=end_time,\n    )\n    \n    df = data.to_df(tz=tz).sort_values(\"ts_event\")\n    print(f\"  [API] Fetched {len(df)} records\")\n    \n    # Save to cache\n    save_to_cache(df, cache_name)\n    \n    return df\n\n\ndef fetch_equity_price(ticker, date):\n    \"\"\"Fetch underlying price at 15:45 ET, with caching\"\"\"\n    cache_name = f\"equity_{ticker}_{date}\"\n    \n    # Try cache first\n    cached = load_from_cache(cache_name)\n    if cached is not None:\n        return cached['close'].iloc[0]\n    \n    # Fetch from API\n    print(f\"  [API] Fetching equity price for {ticker} on {date}...\")\n    \n    tz = CONFIG['timezone']\n    start_time = pd.Timestamp(f\"{date} 15:45\", tz=tz)\n    end_time = start_time + pd.Timedelta(minutes=1)\n    \n    data = client.timeseries.get_range(\n        dataset='XNAS.ITCH',\n        symbols=[ticker],\n        schema='ohlcv-1m',\n        start=start_time,\n        end=end_time,\n        stype_in='raw_symbol'\n    )\n    \n    df = data.to_df()\n    print(f\"  [API] Fetched equity price: ${df['close'].iloc[0]:.2f}\")\n    \n    # Save to cache\n    save_to_cache(df, cache_name)\n    \n    return df['close'].iloc[0]\n\n\ndef fetch_option_daily_ohlcv(symbol, start_date, end_date):\n    \"\"\"Fetch daily OHLCV for an option symbol, with caching\"\"\"\n    # Clean symbol for cache filename\n    cache_name = f\"daily_{symbol.replace(' ', '_')}_{start_date}_{end_date}\"\n    \n    # Try cache first\n    cached = load_from_cache(cache_name)\n    if cached is not None:\n        return cached\n    \n    # Fetch from API\n    print(f\"  [API] Fetching daily OHLCV for {symbol} from {start_date} to {end_date}...\")\n    \n    data = client.timeseries.get_range(\n        dataset='OPRA.PILLAR',\n        schema='ohlcv-1d',\n        symbols=symbol,\n        stype_in='raw_symbol',\n        start=start_date,\n        end=end_date,\n    )\n    \n    df = data.to_df(tz=CONFIG['timezone'])\n    print(f\"  [API] Fetched {len(df)} daily records\")\n    \n    # Save to cache\n    save_to_cache(df, cache_name)\n    \n    return df\n\n\ndef fetch_option_1545_price(symbol, date):\n    \"\"\"Fetch option price at 15:45 ET for a specific date, with caching\"\"\"\n    # Clean symbol for cache filename\n    cache_name = f\"option_1545_{symbol.replace(' ', '_')}_{date}\"\n    \n    # Try cache first\n    cached = load_from_cache(cache_name)\n    if cached is not None:\n        return cached['close'].iloc[0]\n    \n    # Fetch from API\n    print(f\"  [API] Fetching 15:45 price for {symbol} on {date}...\")\n    \n    exit_time = pd.Timestamp(date).tz_localize(CONFIG['timezone']).replace(hour=15, minute=45)\n    \n    data = client.timeseries.get_range(\n        dataset='OPRA.PILLAR',\n        schema='ohlcv-1m',\n        symbols=symbol,\n        stype_in='raw_symbol',\n        start=exit_time,\n        end=exit_time + pd.Timedelta(minutes=1),\n    )\n    \n    df = data.to_df(tz=CONFIG['timezone'])\n    \n    if len(df) > 0:\n        exit_price = df.iloc[0]['close']\n        print(f\"  [API] Fetched price: ${exit_price:.2f}\")\n        \n        # Save to cache\n        save_to_cache(df, cache_name)\n        \n        return exit_price\n    else:\n        print(f\"  [API] No data available\")\n        return None\n\n\nprint(\"Data fetch functions defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "3gtj139e42",
   "source": "## 5b. Technical Filter Functions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "r77z5x24u6b",
   "source": "def fetch_equity_history(ticker, end_date, lookback_days=60):\n    \"\"\"Fetch daily equity OHLCV data for technical analysis, with caching\"\"\"\n    # Calculate start date with buffer for lookback\n    end_dt = pd.Timestamp(end_date)\n    start_dt = end_dt - pd.Timedelta(days=lookback_days)\n    \n    cache_name = f\"equity_daily_{ticker}_{start_dt.date()}_{end_dt.date()}\"\n    \n    # Try cache first\n    cached = load_from_cache(cache_name)\n    if cached is not None:\n        return cached\n    \n    # Fetch from API\n    print(f\"  [API] Fetching equity history for {ticker} from {start_dt.date()} to {end_dt.date()}...\")\n    \n    data = client.timeseries.get_range(\n        dataset='XNAS.ITCH',\n        symbols=[ticker],\n        schema='ohlcv-1d',\n        start=start_dt,\n        end=end_dt + pd.Timedelta(days=1),\n        stype_in='raw_symbol'\n    )\n    \n    df = data.to_df(tz=CONFIG['timezone'])\n    print(f\"  [API] Fetched {len(df)} daily records\")\n    \n    # Save to cache\n    save_to_cache(df, cache_name)\n    \n    return df\n\n\ndef calculate_bollinger_bands(df, window=20, k=2.0):\n    \"\"\"Calculate Bollinger Bands and SMA on equity data\"\"\"\n    df_bb = df.copy().sort_index()\n    \n    # Rolling stats on close\n    roll = df_bb[\"close\"].rolling(window=window, min_periods=window)\n    df_bb[\"sma\"] = roll.mean()\n    df_bb[\"std\"] = roll.std(ddof=0)\n    \n    # Bollinger Bands\n    df_bb[\"bb_upper\"] = df_bb[\"sma\"] + k * df_bb[\"std\"]\n    df_bb[\"bb_lower\"] = df_bb[\"sma\"] - k * df_bb[\"std\"]\n    \n    # Bollinger %B (position within bands)\n    df_bb[\"bb_pctb\"] = (df_bb[\"close\"] - df_bb[\"bb_lower\"]) / (df_bb[\"bb_upper\"] - df_bb[\"bb_lower\"])\n    \n    return df_bb\n\n\ndef check_technical_entry(ticker, entry_date, config):\n    \"\"\"\n    Check if the technical entry conditions are met for a given date.\n    \n    Returns: (passes_filter: bool, details: dict)\n    \"\"\"\n    if not config.get('technical_filter_enabled', False):\n        return True, {'filter_enabled': False}\n    \n    # Need extra lookback for BB calculation\n    lookback_days = config.get('bb_window', 20) + 40\n    \n    # Fetch equity history\n    df_equity = fetch_equity_history(ticker, entry_date, lookback_days)\n    \n    # Calculate Bollinger Bands\n    window = config.get('bb_window', 20)\n    k = config.get('bb_std', 2.0)\n    df_bb = calculate_bollinger_bands(df_equity, window=window, k=k)\n    \n    # Get the entry date row\n    entry_dt = pd.Timestamp(entry_date).tz_localize(CONFIG['timezone']).normalize()\n    \n    # Find the closest date (in case entry_date is exact match or close)\n    df_bb_dates = df_bb.index.normalize()\n    \n    # Try to find an exact or near match\n    mask = df_bb_dates <= entry_dt\n    if not mask.any():\n        print(f\"  [TECH FILTER] No data found for {entry_date}\")\n        return False, {'error': 'no_data'}\n    \n    # Get the most recent row on or before entry_date\n    closest_idx = df_bb[mask].index[-1]\n    row = df_bb.loc[closest_idx]\n    \n    close = row['close']\n    sma = row['sma']\n    bb_lower = row['bb_lower']\n    \n    # Check if we have valid BB data\n    if pd.isna(sma) or pd.isna(bb_lower):\n        print(f\"  [TECH FILTER] Insufficient data for BB calculation on {entry_date}\")\n        return False, {'error': 'insufficient_data'}\n    \n    # Check entry conditions\n    sma_entry = close <= sma\n    bb_entry = close <= bb_lower\n    \n    details = {\n        'date': closest_idx,\n        'close': close,\n        'sma': sma,\n        'bb_lower': bb_lower,\n        'sma_entry': sma_entry,\n        'bb_entry': bb_entry,\n    }\n    \n    # Determine if we pass the filter\n    require_sma = config.get('require_sma_entry', True)\n    require_bb = config.get('require_bb_entry', False)\n    \n    passes = False\n    if require_bb:\n        passes = bb_entry\n    elif require_sma:\n        passes = sma_entry\n    else:\n        passes = sma_entry or bb_entry  # Either condition\n    \n    return passes, details\n\n\nprint(\"Technical filter functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Exit Strategy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exit-strategy",
   "metadata": {},
   "outputs": [],
   "source": "def backtest_exit_strategy(backtest_candidates, client, config):\n    \"\"\"\n    Backtest exit strategy for wheel options\n    \n    Exit conditions:\n    1. Profit target: Exit when mid-price <= 50% of premium (early exit)\n       - If daily range contains exit_50_perc, assume we exited at that exact price\n    2. Time limit: Force exit at 21 DTE using 15:45 ET price\n    \"\"\"\n    exits = []\n    \n    for idx, row in backtest_candidates.iterrows():\n        symbol = row['symbol']\n        \n        # Normalize dates\n        entry_date = pd.Timestamp(row['date']).tz_localize(None)\n        expiration = pd.Timestamp(row['expiration']).tz_localize(None)\n        date_21dte = pd.Timestamp(row['date_21dte']).tz_localize(None)\n        \n        # Entry details\n        premium = row['mid']\n        exit_50_perc = premium * 0.50\n        cost_basis = row['strike'] * 100\n        \n        print(f\"\\nProcessing {symbol}...\")\n        print(f\"  Entry: {entry_date.date()}, Premium: ${premium:.2f}\")\n        print(f\"  Exit target: ${exit_50_perc:.2f} (50%)\")\n        print(f\"  21 DTE date: {date_21dte.date()}\")\n        \n        try:\n            # Fetch daily prices for monitoring (WITH CACHING)\n            start_daily = entry_date + pd.Timedelta(days=1)\n            end_daily = date_21dte\n            \n            df_daily = fetch_option_daily_ohlcv(symbol, start_daily, end_daily)\n            \n            # Check daily for profit target\n            profit_target_hit = False\n            \n            for check_date, daily_row in df_daily.iterrows():\n                daily_low = daily_row['low']\n                daily_high = daily_row['high']\n                \n                # Check if exit target is within daily range\n                if daily_low <= exit_50_perc <= daily_high:\n                    exits.append({\n                        'symbol': symbol,\n                        'entry_date': entry_date,\n                        'exit_date': check_date.tz_localize(None),\n                        'expiration': expiration,\n                        'cost_basis': cost_basis,\n                        'premium': premium,\n                        'exit_50_perc': exit_50_perc,\n                        'exit_price': exit_50_perc,\n                        'exit_reason': 'profit_target',\n                        'days_held': (check_date.tz_localize(None) - entry_date).days,\n                        'daily_low': daily_low,\n                        'daily_high': daily_high,\n                    })\n                    \n                    print(f\"  Profit target hit on {check_date.date()} @ ${exit_50_perc:.2f}\")\n                    print(f\"    (Daily range: ${daily_low:.2f} - ${daily_high:.2f})\")\n                    profit_target_hit = True\n                    break\n            \n            # If profit target not hit, force exit at 21 DTE (WITH CACHING)\n            if not profit_target_hit:\n                exit_price = fetch_option_1545_price(symbol, date_21dte.date())\n                \n                if exit_price is not None:\n                    exits.append({\n                        'symbol': symbol,\n                        'entry_date': entry_date,\n                        'exit_date': date_21dte,\n                        'expiration': expiration,\n                        'cost_basis': cost_basis,\n                        'premium': premium,\n                        'exit_50_perc': exit_50_perc,\n                        'exit_price': exit_price,\n                        'exit_reason': 'time_limit_21dte',\n                        'days_held': (date_21dte - entry_date).days,\n                        'daily_low': None,\n                        'daily_high': None,\n                    })\n                    \n                    print(f\"  Time limit exit on {date_21dte.date()} @ 15:45 ET: ${exit_price:.2f}\")\n                else:\n                    print(f\"  No data for 21 DTE exit\")\n                    \n        except Exception as e:\n            print(f\"  Error: {e}\")\n            import traceback\n            traceback.print_exc()\n            continue\n    \n    # Create results DataFrame\n    exits_df = pd.DataFrame(exits)\n    \n    # Calculate P&L\n    if len(exits_df) > 0:\n        exits_df['exit_pnl'] = exits_df['premium'] - exits_df['exit_price']\n        exits_df['exit_pnl_pct'] = (exits_df['exit_pnl'] / exits_df['premium']) * 100\n        exits_df['roc'] = (exits_df['exit_pnl'] / exits_df['cost_basis']) * 100\n    \n    return exits_df\n\n\nprint(\"Exit strategy function defined\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. Process Entry Date Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "process-date",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entry_date(entry_date, ticker, positions_df, config):\n",
    "    \"\"\"\n",
    "    Process a single entry date:\n",
    "    1. Fetch options chain\n",
    "    2. Parse symbols, calc DTE, IV, delta\n",
    "    3. Filter candidates\n",
    "    4. Remove same-day duplicates (symbols already in positions_df for this date)\n",
    "    5. Run exit strategy\n",
    "    \n",
    "    Returns: (new_positions_df, exits_df)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing entry date: {entry_date}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    r = config['risk_free_rate']\n",
    "    \n",
    "    # 1. Fetch options chain\n",
    "    df_opts = fetch_options_snapshot(ticker, entry_date)\n",
    "    \n",
    "    # 2. Parse symbols\n",
    "    df_opts = parse_option_symbols(df_opts)\n",
    "    \n",
    "    # 3. Add DTE\n",
    "    df_opts = add_trading_dte(df_opts)\n",
    "    \n",
    "    # 4. Fetch underlying price\n",
    "    underlying_price = fetch_equity_price(ticker, entry_date)\n",
    "    \n",
    "    # 5. Keep only rows with quotes\n",
    "    quotes = df_opts[df_opts[\"bid_px_00\"].notna() & df_opts[\"ask_px_00\"].notna()].copy()\n",
    "    quotes[\"mid\"] = (quotes[\"bid_px_00\"] + quotes[\"ask_px_00\"]) / 2\n",
    "    \n",
    "    # 6. Collapse to one row per contract (latest quote)\n",
    "    chain_snapshot = (\n",
    "        quotes\n",
    "        .sort_values(\"ts_event\")\n",
    "        .groupby([\"symbol\", \"expiration\", \"strike\", \"call_put\"])\n",
    "        .tail(1)\n",
    "        .copy()\n",
    "    )\n",
    "    chain_snapshot[\"underlying_last\"] = underlying_price\n",
    "    \n",
    "    # 7. Calculate IV and delta\n",
    "    chain_snapshot[\"iv\"] = chain_snapshot.apply(lambda row: compute_iv(row, r), axis=1)\n",
    "    chain_snapshot[\"delta\"] = chain_snapshot.apply(lambda row: compute_delta(row, r), axis=1)\n",
    "    \n",
    "    # 8. Calculate 21 DTE dates\n",
    "    chain_snapshot['date_21dte'] = calculate_21dte_dates(chain_snapshot['expiration'])\n",
    "    \n",
    "    # 9. Add entry date\n",
    "    chain_snapshot['date'] = chain_snapshot['ts_event'].dt.date\n",
    "    \n",
    "    # 10. Filter candidates\n",
    "    candidates = chain_snapshot[\n",
    "        (chain_snapshot[\"call_put\"] == config['option_type'])\n",
    "        & chain_snapshot[\"dte\"].between(config['min_dte'], config['max_dte'])\n",
    "        & chain_snapshot[\"delta\"].abs().between(config['min_delta'], config['max_delta'])\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nFound {len(candidates)} candidates passing filters\")\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 11. Filter out same-day duplicates (symbols already held from this entry date)\n",
    "    if len(positions_df) > 0:\n",
    "        # Only filter if we have positions from the SAME entry date\n",
    "        same_date_positions = positions_df[positions_df['entry_date'] == entry_date]\n",
    "        if len(same_date_positions) > 0:\n",
    "            held_symbols = same_date_positions['symbol'].tolist()\n",
    "            candidates = candidates[~candidates['symbol'].isin(held_symbols)]\n",
    "            print(f\"After removing same-day duplicates: {len(candidates)} candidates\")\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 12. Create backtest candidates\n",
    "    backtest_candidates = candidates.copy()\n",
    "    backtest_candidates['cost_basis'] = backtest_candidates['underlying_last'] * 100 - backtest_candidates['mid']\n",
    "    backtest_candidates['premium'] = backtest_candidates['mid']\n",
    "    backtest_candidates['exit_50_perc'] = 0.5 * backtest_candidates['premium']\n",
    "    backtest_candidates = backtest_candidates[[\n",
    "        'symbol', 'date_21dte', 'cost_basis', 'premium', 'exit_50_perc',\n",
    "        'date', 'dte', 'expiration', 'mid', 'strike'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"\\nBacktest candidates:\")\n",
    "    print(backtest_candidates[['symbol', 'strike', 'dte', 'premium']].to_string())\n",
    "    \n",
    "    # 13. Run exit strategy\n",
    "    exits_df = backtest_exit_strategy(backtest_candidates, client, config)\n",
    "    \n",
    "    # 14. Create new positions (entry info for tracking)\n",
    "    new_positions = backtest_candidates.copy()\n",
    "    new_positions['entry_date'] = entry_date\n",
    "    \n",
    "    return new_positions, exits_df\n",
    "\n",
    "\n",
    "print(\"Process entry date function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Main Backtest Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "main-loop",
   "metadata": {},
   "outputs": [],
   "source": "# Initialize tracking DataFrames\npositions_df = pd.DataFrame()  # Active positions\nall_exits_df = pd.DataFrame()  # All completed trades\nskipped_dates = []  # Dates that failed technical filter\n\n# Process each entry date\nfor entry_date in CONFIG['entry_dates']:\n    \n    # Check technical filter first\n    if CONFIG.get('technical_filter_enabled', False):\n        passes_filter, tech_details = check_technical_entry(\n            CONFIG['ticker'], entry_date, CONFIG\n        )\n        \n        if not passes_filter:\n            print(f\"\\n[SKIPPED] {entry_date} - Failed technical filter\")\n            if 'close' in tech_details:\n                print(f\"  Close: ${tech_details['close']:.2f}, SMA: ${tech_details['sma']:.2f}, BB Lower: ${tech_details['bb_lower']:.2f}\")\n            skipped_dates.append({'date': entry_date, 'reason': 'technical_filter', **tech_details})\n            continue\n        else:\n            print(f\"\\n[PASSED] {entry_date} - Technical filter passed\")\n            if 'close' in tech_details:\n                print(f\"  Close: ${tech_details['close']:.2f}, SMA: ${tech_details['sma']:.2f}\")\n                print(f\"  SMA entry: {tech_details['sma_entry']}, BB entry: {tech_details['bb_entry']}\")\n    \n    # Process this entry date\n    new_positions, exits = process_entry_date(\n        entry_date=entry_date,\n        ticker=CONFIG['ticker'],\n        positions_df=positions_df,\n        config=CONFIG\n    )\n    \n    # Add new positions\n    if len(new_positions) > 0:\n        positions_df = pd.concat([positions_df, new_positions], ignore_index=True)\n    \n    # Accumulate exits\n    if len(exits) > 0:\n        all_exits_df = pd.concat([all_exits_df, exits], ignore_index=True)\n\nprint(f\"\\n{'='*60}\")\nprint(\"BACKTEST COMPLETE\")\nprint('='*60)\nprint(f\"Total dates processed: {len(CONFIG['entry_dates'])}\")\nif CONFIG.get('technical_filter_enabled', False):\n    print(f\"Dates passing technical filter: {len(CONFIG['entry_dates']) - len(skipped_dates)}\")\n    print(f\"Dates skipped (failed filter): {len(skipped_dates)}\")\nprint(f\"Total positions entered: {len(positions_df)}\")\nprint(f\"Total exits: {len(all_exits_df)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_exits_df) > 0:\n",
    "    print(\"\\nExit reasons:\")\n",
    "    print(all_exits_df['exit_reason'].value_counts())\n",
    "    \n",
    "    print(\"\\nP&L Summary:\")\n",
    "    print(all_exits_df[['exit_pnl', 'exit_pnl_pct', 'roc']].describe())\n",
    "    \n",
    "    print(\"\\nAll exits:\")\n",
    "    display_cols = ['symbol', 'entry_date', 'exit_date', 'premium', 'exit_price', \n",
    "                   'exit_pnl', 'roc', 'exit_reason']\n",
    "    print(all_exits_df[display_cols].to_string())\n",
    "else:\n",
    "    print(\"No exits recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. Validation Against Single-Day Notebook\n",
    "\n",
    "Expected results for `entry_dates = ['2023-06-06']` with TSLA:\n",
    "\n",
    "| Symbol | Entry Date | Exit Date | Exit Reason | Premium | Exit Price |\n",
    "|--------|------------|-----------|-------------|---------|------------|\n",
    "| TSLA 230721P00200000 | 2023-06-06 | 2023-06-08 | profit_target | ~$7.85 | ~$3.92 |\n",
    "| TSLA 230721P00205000 | 2023-06-06 | 2023-06-08 | profit_target | ~$9.53 | ~$4.76 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with expected\n",
    "expected_symbols = ['TSLA  230721P00200000', 'TSLA  230721P00205000']\n",
    "\n",
    "if len(all_exits_df) > 0:\n",
    "    print(\"Validation check:\")\n",
    "    for sym in expected_symbols:\n",
    "        match = all_exits_df[all_exits_df['symbol'] == sym]\n",
    "        if len(match) > 0:\n",
    "            row = match.iloc[0]\n",
    "            print(f\"  {sym}:\")\n",
    "            print(f\"    Exit date: {row['exit_date']}\")\n",
    "            print(f\"    Exit reason: {row['exit_reason']}\")\n",
    "            print(f\"    Premium: ${row['premium']:.2f}\")\n",
    "            print(f\"    Exit price: ${row['exit_price']:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {sym}: NOT FOUND\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}