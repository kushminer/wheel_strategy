{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Multi-Symbol Wheel Strategy Backtest\n",
    "\n",
    "This notebook processes multiple symbols and entry dates for the wheel strategy backtest.\n",
    "\n",
    "**Features:**\n",
    "- **Multi-symbol support**: Run backtest across multiple tickers (e.g., TSLA, AAPL)\n",
    "- Caches API responses to minimize requests\n",
    "- Tracks active positions per symbol\n",
    "- Prevents same-day duplicate entries per symbol\n",
    "- Per-symbol and aggregate performance metrics\n",
    "\n",
    "**Validation:**\n",
    "Start with `tickers = ['TSLA']` and `entry_dates = ['2023-06-06']` to compare with single-day notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(\"/Users/samuelminer/Projects/nissan_options/wheel_strategy/.env\")\n",
    "load_dotenv(env_path, override=True)\n",
    "assert os.getenv(\"DATABENTO_API_KEY\"), \"DATABENTO_API_KEY not found\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import databento as db\n",
    "import pandas_market_calendars as mcal\n",
    "from py_vollib.black_scholes.implied_volatility import implied_volatility\n",
    "from py_vollib.black_scholes.greeks.analytical import delta\n",
    "\n",
    "# Initialize clients\n",
    "client = db.Historical()\n",
    "nyse = mcal.get_calendar(\"NYSE\")\n",
    "\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Wheel Strategy Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    # Tickers (multiple symbols supported)\n",
    "    'tickers': ['TSLA', 'AAPL'],\n",
    "\n",
    "    # Entry dates\n",
    "    # 'entry_dates': ['2023-06-06'],  # Start with one date to validate\n",
    "\n",
    "    # OR use date range (will be converted to trading days automatically)\n",
    "    'start_date': '2023-06-01',\n",
    "    'end_date': '2023-06-30',\n",
    "\n",
    "    # Timezone\n",
    "    'timezone': 'America/New_York',\n",
    "\n",
    "    # Cache directory\n",
    "    'cache_dir': '../cache/',\n",
    "\n",
    "    # Technical filter settings\n",
    "    'technical_filter_enabled': False,  # Set to False to disable\n",
    "    'bb_window': 20,                   # Bollinger Band lookback period\n",
    "    'bb_std': 2.0,                     # Bollinger Band standard deviations\n",
    "    'require_sma_entry': False,         # Entry when close <= SMA20\n",
    "    'require_bb_entry': False,         # Entry when close <= lower BB (more restrictive)\n",
    "\n",
    "    # Option filters\n",
    "    'min_dte': 30,\n",
    "    'max_dte': 45,\n",
    "    'min_delta': 0.25,\n",
    "    'max_delta': 0.35,\n",
    "    'option_type': 'P',\n",
    "\n",
    "    # Exit strategy\n",
    "    'profit_target_pct': 0.50,\n",
    "    'exit_dte': 21,\n",
    "\n",
    "    # DTE calculation settings\n",
    "    'use_trading_days_for_dte_filter': True,   # True = trading days, False = calendar days (for min_dte/max_dte filtering)\n",
    "    'use_trading_days_for_exit': True,         # True = trading days, False = calendar days (for exit date calculation)\n",
    "\n",
    "    # Risk-free rate for IV/delta calculation\n",
    "    'risk_free_rate': 0.04,\n",
    "}\n",
    "\n",
    "# Create cache directory if needed\n",
    "os.makedirs(CONFIG['cache_dir'], exist_ok=True)\n",
    "\n",
    "# Convert date range to list of trading days if specified\n",
    "if 'start_date' in CONFIG and 'end_date' in CONFIG:\n",
    "    start = pd.Timestamp(CONFIG['start_date'])\n",
    "    end = pd.Timestamp(CONFIG['end_date'])\n",
    "\n",
    "    # Get trading days from NYSE calendar\n",
    "    trading_days = nyse.valid_days(start_date=start, end_date=end)\n",
    "    CONFIG['entry_dates'] = [d.strftime('%Y-%m-%d') for d in trading_days]\n",
    "\n",
    "    print(f\"Date range: {CONFIG['start_date']} to {CONFIG['end_date']}\")\n",
    "    print(f\"Generated {len(CONFIG['entry_dates'])} trading days\")\n",
    "    print(f\"First 5: {CONFIG['entry_dates'][:5]}\")\n",
    "    print(f\"Last 5: {CONFIG['entry_dates'][-5:]}\")\n",
    "else:\n",
    "    print(f\"Entry dates: {CONFIG['entry_dates']}\")\n",
    "\n",
    "print(f\"Tickers: {CONFIG['tickers']}\")\n",
    "print(f\"Cache dir: {CONFIG['cache_dir']}\")\n",
    "print(f\"Technical filter: {'ENABLED' if CONFIG['technical_filter_enabled'] else 'DISABLED'}\")\n",
    "if CONFIG['technical_filter_enabled']:\n",
    "    print(f\"  SMA entry (close <= SMA{CONFIG['bb_window']}): {CONFIG['require_sma_entry']}\")\n",
    "    print(f\"  BB entry (close <= lower BB): {CONFIG['require_bb_entry']}\")\n",
    "print(f\"\\nDTE settings:\")\n",
    "print(f\"  Filter DTE: {'trading days' if CONFIG['use_trading_days_for_dte_filter'] else 'calendar days'}\")\n",
    "print(f\"  Exit DTE: {'trading days' if CONFIG['use_trading_days_for_exit'] else 'calendar days'}\")\n",
    "print(f\"\\nTotal combinations: {len(CONFIG['tickers'])} tickers x {len(CONFIG['entry_dates'])} dates = {len(CONFIG['tickers']) * len(CONFIG['entry_dates'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Caching Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caching",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cache_path(name):\n",
    "    \"\"\"Get full path for a cache file\"\"\"\n",
    "    return os.path.join(CONFIG['cache_dir'], f\"{name}.parquet\")\n",
    "\n",
    "def load_from_cache(name):\n",
    "    \"\"\"Load DataFrame from cache if it exists\"\"\"\n",
    "    path = get_cache_path(name)\n",
    "    if os.path.exists(path):\n",
    "        print(f\"  [CACHE HIT] Loading {name}\")\n",
    "        return pd.read_parquet(path)\n",
    "    return None\n",
    "\n",
    "def save_to_cache(df, name):\n",
    "    \"\"\"Save DataFrame to cache\"\"\"\n",
    "    path = get_cache_path(name)\n",
    "    df.to_parquet(path)\n",
    "    print(f\"  [CACHE SAVE] Saved {name}\")\n",
    "\n",
    "print(\"Caching functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_option_symbols(df):\n",
    "    \"\"\"Parse OPRA symbols into components\"\"\"\n",
    "    sym = df[\"symbol\"]\n",
    "    \n",
    "    # Split ROOT and OPRA code\n",
    "    root_and_code = sym.str.split(expand=True)\n",
    "    df[\"root\"] = root_and_code[0]\n",
    "    code = root_and_code[1]\n",
    "    \n",
    "    # Expiration: YYMMDD\n",
    "    df[\"expiration\"] = pd.to_datetime(code.str[:6], format=\"%y%m%d\")\n",
    "    \n",
    "    # Call/Put flag\n",
    "    df[\"call_put\"] = code.str[6]\n",
    "    \n",
    "    # Strike: in 1/1000 dollars\n",
    "    strike_int = code.str[7:].astype(\"int32\")\n",
    "    df[\"strike\"] = strike_int / 1000.0\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_trading_dte(df, tz=\"America/New_York\", use_trading_days=True):\n",
    "    \"\"\"\n",
    "    Add days-to-expiration using NYSE calendar or calendar days.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with ts_event and expiration columns\n",
    "        tz: Timezone for event dates\n",
    "        use_trading_days: If True, count trading days. If False, count calendar days.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "    \n",
    "    # Event dates from ts_event column\n",
    "    event_dt = pd.to_datetime(out[\"ts_event\"]).dt.tz_convert(tz).dt.normalize()\n",
    "    event_days = pd.to_datetime(event_dt.dt.date)  # tz-naive\n",
    "    \n",
    "    # Expiration dates\n",
    "    exp_dt = pd.to_datetime(out[\"expiration\"])\n",
    "    exp_days = pd.to_datetime(exp_dt.dt.date)  # tz-naive\n",
    "    \n",
    "    if use_trading_days:\n",
    "        # Build trading calendar\n",
    "        start_date = event_days.min().date()\n",
    "        end_date = exp_days.max().date()\n",
    "        \n",
    "        schedule = nyse.valid_days(start_date=start_date, end_date=end_date)\n",
    "        schedule = pd.to_datetime(schedule).normalize().tz_localize(None)\n",
    "        \n",
    "        cal_index = pd.Series(np.arange(len(schedule), dtype=np.int32), index=schedule)\n",
    "        \n",
    "        event_idx = cal_index.reindex(event_days).to_numpy()\n",
    "        exp_idx = cal_index.reindex(exp_days).to_numpy()\n",
    "        \n",
    "        out[\"dte\"] = (exp_idx - event_idx - 1).astype(np.int16)\n",
    "    else:\n",
    "        # Calendar days (simple subtraction)\n",
    "        out[\"dte\"] = (exp_days - event_days).dt.days.astype(np.int16)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def calculate_exit_dte_dates(expirations, exit_dte, use_trading_days=True):\n",
    "    \"\"\"\n",
    "    Calculate dates at exit_dte days before expiration.\n",
    "    \n",
    "    Args:\n",
    "        expirations: Series of expiration dates\n",
    "        exit_dte: Number of days before expiration to exit (0 = expiration day)\n",
    "        use_trading_days: If True, count trading days. If False, count calendar days\n",
    "                          (will adjust to prior market day if lands on non-trading day)\n",
    "    \n",
    "    Returns:\n",
    "        Series of exit dates (always valid trading days)\n",
    "    \"\"\"\n",
    "    if exit_dte == 0:\n",
    "        # Exit at expiration - return expiration dates (already trading days for options)\n",
    "        return expirations.apply(lambda x: pd.Timestamp(x).normalize())\n",
    "    \n",
    "    min_exp = expirations.min()\n",
    "    max_exp = expirations.max()\n",
    "    \n",
    "    start_date = min_exp - pd.Timedelta(days=60)\n",
    "    end_date = max_exp\n",
    "    \n",
    "    schedule = nyse.schedule(start_date=start_date, end_date=end_date)\n",
    "    trading_days = schedule.index.tz_localize(None)\n",
    "    trading_days_set = set(trading_days)\n",
    "    \n",
    "    results = []\n",
    "    for exp in expirations:\n",
    "        exp_dt = pd.Timestamp(exp).normalize()\n",
    "        \n",
    "        if use_trading_days:\n",
    "            # Count back trading days\n",
    "            valid_days = trading_days[trading_days <= exp_dt]\n",
    "            \n",
    "            if len(valid_days) >= exit_dte:\n",
    "                target_date = valid_days[-(exit_dte)]\n",
    "            else:\n",
    "                target_date = valid_days[0] if len(valid_days) > 0 else exp_dt\n",
    "        else:\n",
    "            # Count back calendar days\n",
    "            target_date = exp_dt - pd.Timedelta(days=exit_dte)\n",
    "            \n",
    "            # If target_date is not a trading day, find the prior trading day\n",
    "            if target_date not in trading_days_set:\n",
    "                prior_days = trading_days[trading_days < target_date]\n",
    "                if len(prior_days) > 0:\n",
    "                    target_date = prior_days[-1]\n",
    "                else:\n",
    "                    # Fallback to expiration if no prior trading days\n",
    "                    target_date = exp_dt\n",
    "        \n",
    "        results.append(target_date)\n",
    "    \n",
    "    return pd.Series(results, index=expirations.index)\n",
    "\n",
    "\n",
    "def compute_iv(row, r):\n",
    "    \"\"\"Compute implied volatility\"\"\"\n",
    "    price = row[\"mid\"]\n",
    "    S = row[\"underlying_last\"]\n",
    "    K = row[\"strike\"]\n",
    "    t = row[\"dte\"] / 365.0\n",
    "    flag = \"p\" if row[\"call_put\"] == \"P\" else \"c\"\n",
    "\n",
    "    if not (np.isfinite(price) and np.isfinite(S) and np.isfinite(K) and t > 0):\n",
    "        return np.nan\n",
    "    if price <= 0 or S <= 0 or K <= 0:\n",
    "        return np.nan\n",
    "\n",
    "    try:\n",
    "        return implied_volatility(price, S, K, t, r, flag)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def compute_delta(row, r):\n",
    "    \"\"\"Compute delta using IV\"\"\"\n",
    "    sigma = row[\"iv\"]\n",
    "    if not np.isfinite(sigma):\n",
    "        return np.nan\n",
    "\n",
    "    S = row[\"underlying_last\"]\n",
    "    K = row[\"strike\"]\n",
    "    t = row[\"dte\"] / 365.0\n",
    "    flag = \"p\" if row[\"call_put\"] == \"P\" else \"c\"\n",
    "\n",
    "    return delta(flag, S, K, t, r, sigma)\n",
    "\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5",
   "metadata": {},
   "source": [
    "## 5. Data Fetch Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fetch-functions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_options_snapshot(ticker, date):\n",
    "    \"\"\"Fetch option chain at 15:45 ET, with caching\"\"\"\n",
    "    cache_name = f\"options_{ticker}_{date}\"\n",
    "    \n",
    "    # Try cache first\n",
    "    cached = load_from_cache(cache_name)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(f\"  [API] Fetching options for {ticker} on {date}...\")\n",
    "    \n",
    "    tz = CONFIG['timezone']\n",
    "    start_time = pd.Timestamp(f\"{date} 15:45\", tz=tz)\n",
    "    end_time = start_time + pd.Timedelta(minutes=1)\n",
    "    \n",
    "    data = client.timeseries.get_range(\n",
    "        dataset='OPRA.PILLAR',\n",
    "        schema='cmbp-1',\n",
    "        symbols=f'{ticker}.OPT',\n",
    "        stype_in='parent',\n",
    "        start=start_time,\n",
    "        end=end_time,\n",
    "    )\n",
    "    \n",
    "    df = data.to_df(tz=tz).sort_values(\"ts_event\")\n",
    "    print(f\"  [API] Fetched {len(df)} records\")\n",
    "    \n",
    "    # Save to cache\n",
    "    save_to_cache(df, cache_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_equity_price(ticker, date):\n",
    "    \"\"\"Fetch underlying price at 15:45 ET, with caching\"\"\"\n",
    "    cache_name = f\"equity_{ticker}_{date}\"\n",
    "    \n",
    "    # Try cache first\n",
    "    cached = load_from_cache(cache_name)\n",
    "    if cached is not None:\n",
    "        return cached['close'].iloc[0]\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(f\"  [API] Fetching equity price for {ticker} on {date}...\")\n",
    "    \n",
    "    tz = CONFIG['timezone']\n",
    "    start_time = pd.Timestamp(f\"{date} 15:45\", tz=tz)\n",
    "    end_time = start_time + pd.Timedelta(minutes=1)\n",
    "    \n",
    "    data = client.timeseries.get_range(\n",
    "        dataset='XNAS.ITCH',\n",
    "        symbols=[ticker],\n",
    "        schema='ohlcv-1m',\n",
    "        start=start_time,\n",
    "        end=end_time,\n",
    "        stype_in='raw_symbol'\n",
    "    )\n",
    "    \n",
    "    df = data.to_df()\n",
    "    print(f\"  [API] Fetched equity price: ${df['close'].iloc[0]:.2f}\")\n",
    "    \n",
    "    # Save to cache\n",
    "    save_to_cache(df, cache_name)\n",
    "    \n",
    "    return df['close'].iloc[0]\n",
    "\n",
    "\n",
    "def fetch_option_daily_ohlcv(symbol, start_date, end_date):\n",
    "    \"\"\"Fetch daily OHLCV for an option symbol, with caching\"\"\"\n",
    "    # Clean symbol for cache filename\n",
    "    cache_name = f\"daily_{symbol.replace(' ', '_')}_{start_date}_{end_date}\"\n",
    "    \n",
    "    # Try cache first\n",
    "    cached = load_from_cache(cache_name)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(f\"  [API] Fetching daily OHLCV for {symbol} from {start_date} to {end_date}...\")\n",
    "    \n",
    "    data = client.timeseries.get_range(\n",
    "        dataset='OPRA.PILLAR',\n",
    "        schema='ohlcv-1d',\n",
    "        symbols=symbol,\n",
    "        stype_in='raw_symbol',\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "    )\n",
    "    \n",
    "    df = data.to_df(tz=CONFIG['timezone'])\n",
    "    print(f\"  [API] Fetched {len(df)} daily records\")\n",
    "    \n",
    "    # Save to cache\n",
    "    save_to_cache(df, cache_name)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_option_1545_price(symbol, date):\n",
    "    \"\"\"Fetch option price at 15:45 ET for a specific date, with caching\"\"\"\n",
    "    # Clean symbol for cache filename\n",
    "    cache_name = f\"option_1545_{symbol.replace(' ', '_')}_{date}\"\n",
    "    \n",
    "    # Try cache first\n",
    "    cached = load_from_cache(cache_name)\n",
    "    if cached is not None:\n",
    "        return cached['close'].iloc[0]\n",
    "    \n",
    "    # Fetch from API\n",
    "    print(f\"  [API] Fetching 15:45 price for {symbol} on {date}...\")\n",
    "    \n",
    "    exit_time = pd.Timestamp(date).tz_localize(CONFIG['timezone']).replace(hour=15, minute=45)\n",
    "    \n",
    "    data = client.timeseries.get_range(\n",
    "        dataset='OPRA.PILLAR',\n",
    "        schema='ohlcv-1m',\n",
    "        symbols=symbol,\n",
    "        stype_in='raw_symbol',\n",
    "        start=exit_time,\n",
    "        end=exit_time + pd.Timedelta(minutes=1),\n",
    "    )\n",
    "    \n",
    "    df = data.to_df(tz=CONFIG['timezone'])\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        exit_price = df.iloc[0]['close']\n",
    "        print(f\"  [API] Fetched price: ${exit_price:.2f}\")\n",
    "        \n",
    "        # Save to cache\n",
    "        save_to_cache(df, cache_name)\n",
    "        \n",
    "        return exit_price\n",
    "    else:\n",
    "        print(f\"  [API] No data available\")\n",
    "        return None\n",
    "\n",
    "\n",
    "print(\"Data fetch functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0xvhk1gbnh7",
   "metadata": {},
   "source": [
    "## 5b. Technical Filter Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loz4txgrwi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_equity_history(ticker, end_date, lookback_days=60):\n",
    "    \"\"\"Fetch daily equity OHLCV data for technical analysis, with caching\"\"\"\n",
    "    # Calculate start date with buffer for lookback\n",
    "    end_dt = pd.Timestamp(end_date)\n",
    "    start_dt = end_dt - pd.Timedelta(days=lookback_days)\n",
    "\n",
    "    cache_name = f\"equity_daily_{ticker}_{start_dt.date()}_{end_dt.date()}\"\n",
    "\n",
    "    # Try cache first\n",
    "    cached = load_from_cache(cache_name)\n",
    "    if cached is not None:\n",
    "        return cached\n",
    "\n",
    "    # Fetch from API\n",
    "    print(f\"  [API] Fetching equity history for {ticker} from {start_dt.date()} to {end_dt.date()}...\")\n",
    "\n",
    "    data = client.timeseries.get_range(\n",
    "        dataset='XNAS.ITCH',\n",
    "        symbols=[ticker],\n",
    "        schema='ohlcv-1d',\n",
    "        start=start_dt,\n",
    "        end=end_dt + pd.Timedelta(days=1),\n",
    "        stype_in='raw_symbol'\n",
    "    )\n",
    "\n",
    "    df = data.to_df(tz=CONFIG['timezone'])\n",
    "    print(f\"  [API] Fetched {len(df)} daily records\")\n",
    "\n",
    "    # Save to cache\n",
    "    save_to_cache(df, cache_name)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_bollinger_bands(df, window=20, k=2.0):\n",
    "    \"\"\"Calculate Bollinger Bands and SMA on equity data\"\"\"\n",
    "    df_bb = df.copy().sort_index()\n",
    "\n",
    "    # Rolling stats on close\n",
    "    roll = df_bb[\"close\"].rolling(window=window, min_periods=window)\n",
    "    df_bb[\"sma\"] = roll.mean()\n",
    "    df_bb[\"std\"] = roll.std(ddof=0)\n",
    "\n",
    "    # Bollinger Bands\n",
    "    df_bb[\"bb_upper\"] = df_bb[\"sma\"] + k * df_bb[\"std\"]\n",
    "    df_bb[\"bb_lower\"] = df_bb[\"sma\"] - k * df_bb[\"std\"]\n",
    "\n",
    "    # Bollinger %B (position within bands)\n",
    "    df_bb[\"bb_pctb\"] = (df_bb[\"close\"] - df_bb[\"bb_lower\"]) / (df_bb[\"bb_upper\"] - df_bb[\"bb_lower\"])\n",
    "\n",
    "    return df_bb\n",
    "\n",
    "\n",
    "def check_technical_entry(ticker, entry_date, config):\n",
    "    \"\"\"\n",
    "    Check if the technical entry conditions are met for a given date.\n",
    "\n",
    "    Returns: (passes_filter: bool, details: dict)\n",
    "    \"\"\"\n",
    "    if not config.get('technical_filter_enabled', False):\n",
    "        return True, {'filter_enabled': False}\n",
    "\n",
    "    # Need extra lookback for BB calculation\n",
    "    lookback_days = config.get('bb_window', 20) + 40\n",
    "\n",
    "    # Fetch equity history\n",
    "    df_equity = fetch_equity_history(ticker, entry_date, lookback_days)\n",
    "\n",
    "    # Calculate Bollinger Bands\n",
    "    window = config.get('bb_window', 20)\n",
    "    k = config.get('bb_std', 2.0)\n",
    "    df_bb = calculate_bollinger_bands(df_equity, window=window, k=k)\n",
    "\n",
    "    # Get the entry date row\n",
    "    entry_dt = pd.Timestamp(entry_date).tz_localize(CONFIG['timezone']).normalize()\n",
    "\n",
    "    # Find the closest date (in case entry_date is exact match or close)\n",
    "    df_bb_dates = df_bb.index.normalize()\n",
    "\n",
    "    # Try to find an exact or near match\n",
    "    mask = df_bb_dates <= entry_dt\n",
    "    if not mask.any():\n",
    "        print(f\"  [TECH FILTER] No data found for {entry_date}\")\n",
    "        return False, {'error': 'no_data'}\n",
    "\n",
    "    # Get the most recent row on or before entry_date\n",
    "    closest_idx = df_bb[mask].index[-1]\n",
    "    row = df_bb.loc[closest_idx]\n",
    "\n",
    "    close = row['close']\n",
    "    sma = row['sma']\n",
    "    bb_lower = row['bb_lower']\n",
    "\n",
    "    # Check if we have valid BB data\n",
    "    if pd.isna(sma) or pd.isna(bb_lower):\n",
    "        print(f\"  [TECH FILTER] Insufficient data for BB calculation on {entry_date}\")\n",
    "        return False, {'error': 'insufficient_data'}\n",
    "\n",
    "    # Check entry conditions\n",
    "    sma_entry = close <= sma\n",
    "    bb_entry = close <= bb_lower\n",
    "\n",
    "    details = {\n",
    "        'date': closest_idx,\n",
    "        'close': close,\n",
    "        'sma': sma,\n",
    "        'bb_lower': bb_lower,\n",
    "        'sma_entry': sma_entry,\n",
    "        'bb_entry': bb_entry,\n",
    "    }\n",
    "\n",
    "    # Determine if we pass the filter\n",
    "    require_sma = config.get('require_sma_entry', True)\n",
    "    require_bb = config.get('require_bb_entry', False)\n",
    "\n",
    "    passes = False\n",
    "    if require_bb:\n",
    "        passes = bb_entry\n",
    "    elif require_sma:\n",
    "        passes = sma_entry\n",
    "    else:\n",
    "        passes = sma_entry or bb_entry  # Either condition\n",
    "\n",
    "    return passes, details\n",
    "\n",
    "\n",
    "print(\"Technical filter functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Exit Strategy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "exit-strategy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exit strategy function defined\n"
     ]
    }
   ],
   "source": [
    "def backtest_exit_strategy(backtest_candidates, ticker, client, config):\n",
    "    \"\"\"\n",
    "    Backtest exit strategy for wheel options\n",
    "    \n",
    "    Exit conditions:\n",
    "    1. Profit target: Exit when mid-price <= 50% of premium (early exit)\n",
    "       - If daily range contains exit_target_price, assume we exited at that exact price\n",
    "    2. Time limit: Force exit at exit_dte using 15:45 ET price\n",
    "    \"\"\"\n",
    "    exits = []\n",
    "    exit_dte = config.get('exit_dte', 21)\n",
    "    \n",
    "    for idx, row in backtest_candidates.iterrows():\n",
    "        symbol = row['symbol']\n",
    "        \n",
    "        # Normalize dates\n",
    "        entry_date = pd.Timestamp(row['date']).tz_localize(None)\n",
    "        expiration = pd.Timestamp(row['expiration']).tz_localize(None)\n",
    "        date_exit = pd.Timestamp(row['date_exit']).tz_localize(None)\n",
    "        \n",
    "        # Entry details\n",
    "        premium = row['mid']\n",
    "        profit_target_pct = CONFIG['profit_target_pct']\n",
    "        exit_target_price = premium * profit_target_pct\n",
    "        cost_basis = row['strike'] * 100\n",
    "        \n",
    "        print(f\"\\nProcessing {symbol}...\")\n",
    "        print(f\"  Entry: {entry_date.date()}, Premium: ${premium:.2f}\")\n",
    "        print(f\"  Exit target: ${exit_target_price:.2f} ({CONFIG['profit_target_pct']*100}%)\")\n",
    "        print(f\"  Exit date ({exit_dte} DTE): {date_exit.date()}\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch daily prices for monitoring (WITH CACHING)\n",
    "            start_daily = entry_date + pd.Timedelta(days=1)\n",
    "            end_daily = date_exit\n",
    "            \n",
    "            # Validate date range\n",
    "            if start_daily > end_daily:\n",
    "                print(f\"  Warning: Invalid date range (entry+1 > exit), skipping profit target check\")\n",
    "                df_daily = pd.DataFrame()\n",
    "            else:\n",
    "                df_daily = fetch_option_daily_ohlcv(symbol, start_daily, end_daily)\n",
    "            \n",
    "            # Check daily for profit target\n",
    "            profit_target_hit = False\n",
    "            \n",
    "            for check_date, daily_row in df_daily.iterrows():\n",
    "                daily_low = daily_row['low']\n",
    "                daily_high = daily_row['high']\n",
    "                \n",
    "                # Check if exit target is within daily range\n",
    "                if daily_low <= exit_target_price <= daily_high:\n",
    "                    exits.append({\n",
    "                        'ticker': ticker,\n",
    "                        'symbol': symbol,\n",
    "                        'entry_date': entry_date,\n",
    "                        'exit_date': check_date.tz_localize(None),\n",
    "                        'expiration': expiration,\n",
    "                        'cost_basis': cost_basis,\n",
    "                        'premium': premium,\n",
    "                        'profit_target_pct': profit_target_pct,\n",
    "                        'exit_target_price': exit_target_price,\n",
    "                        'exit_reason': 'profit_target',\n",
    "                        'days_held': (check_date.tz_localize(None) - entry_date).days,\n",
    "                        'daily_low': daily_low,\n",
    "                        'daily_high': daily_high,\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  Profit target hit on {check_date.date()} @ ${exit_target_price:.2f}\")\n",
    "                    print(f\"    (Daily range: ${daily_low:.2f} - ${daily_high:.2f})\")\n",
    "                    profit_target_hit = True\n",
    "                    break\n",
    "            \n",
    "            # If profit target not hit, force exit at exit date (WITH CACHING)\n",
    "            if not profit_target_hit:\n",
    "                exit_price = fetch_option_1545_price(symbol, date_exit.date())\n",
    "                \n",
    "                if exit_price is not None:\n",
    "                    exits.append({\n",
    "                        'ticker': ticker,\n",
    "                        'symbol': symbol,\n",
    "                        'entry_date': entry_date,\n",
    "                        'exit_date': date_exit,\n",
    "                        'expiration': expiration,\n",
    "                        'cost_basis': cost_basis,\n",
    "                        'premium': premium,\n",
    "                        'exit_target_price': exit_target_price,\n",
    "                        'exit_price': exit_price,\n",
    "                        'exit_reason': f'time_limit_{exit_dte}dte',\n",
    "                        'days_held': (date_exit - entry_date).days,\n",
    "                        'daily_low': None,\n",
    "                        'daily_high': None,\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"  Time limit exit on {date_exit.date()} @ 15:45 ET: ${exit_price:.2f}\")\n",
    "                else:\n",
    "                    print(f\"  No data for {exit_dte} DTE exit\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  Error: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    exits_df = pd.DataFrame(exits)\n",
    "    \n",
    "    # Calculate P&L\n",
    "    if len(exits_df) > 0:\n",
    "        exits_df['exit_pnl'] = exits_df['premium'] - exits_df['exit_price']\n",
    "        exits_df['exit_pnl_pct'] = (exits_df['exit_pnl'] / exits_df['premium']) * 100\n",
    "        exits_df['roc'] = (exits_df['exit_pnl'] / exits_df['cost_basis']) * 100\n",
    "    \n",
    "    return exits_df\n",
    "\n",
    "\n",
    "print(\"Exit strategy function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. Process Entry Date Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "process-date",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process entry date function defined\n"
     ]
    }
   ],
   "source": [
    "def process_entry_date(entry_date, ticker, positions_df, config):\n",
    "    \"\"\"\n",
    "    Process a single entry date for a single ticker:\n",
    "    1. Fetch options chain\n",
    "    2. Parse symbols, calc DTE, IV, delta\n",
    "    3. Filter candidates\n",
    "    4. Remove same-day duplicates (symbols already in positions_df for this date/ticker)\n",
    "    5. Run exit strategy\n",
    "    \n",
    "    Returns: (new_positions_df, exits_df)\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing {ticker} on {entry_date}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    r = config['risk_free_rate']\n",
    "    use_trading_days_filter = config.get('use_trading_days_for_dte_filter', True)\n",
    "    use_trading_days_exit = config.get('use_trading_days_for_exit', True)\n",
    "    exit_dte = config.get('exit_dte', 21)\n",
    "    \n",
    "    # 1. Fetch options chain\n",
    "    df_opts = fetch_options_snapshot(ticker, entry_date)\n",
    "    \n",
    "    # 2. Parse symbols\n",
    "    df_opts = parse_option_symbols(df_opts)\n",
    "    \n",
    "    # 3. Add DTE (using config setting for trading vs calendar days)\n",
    "    df_opts = add_trading_dte(df_opts, use_trading_days=use_trading_days_filter)\n",
    "    \n",
    "    # 4. Fetch underlying price\n",
    "    underlying_price = fetch_equity_price(ticker, entry_date)\n",
    "    \n",
    "    # 5. Keep only rows with quotes\n",
    "    quotes = df_opts[df_opts[\"bid_px_00\"].notna() & df_opts[\"ask_px_00\"].notna()].copy()\n",
    "    quotes[\"mid\"] = (quotes[\"bid_px_00\"] + quotes[\"ask_px_00\"]) / 2\n",
    "    \n",
    "    # 6. Collapse to one row per contract (latest quote)\n",
    "    chain_snapshot = (\n",
    "        quotes\n",
    "        .sort_values(\"ts_event\")\n",
    "        .groupby([\"symbol\", \"expiration\", \"strike\", \"call_put\"])\n",
    "        .tail(1)\n",
    "        .copy()\n",
    "    )\n",
    "    chain_snapshot[\"underlying_last\"] = underlying_price\n",
    "    \n",
    "    # 7. Calculate IV and delta\n",
    "    chain_snapshot[\"iv\"] = chain_snapshot.apply(lambda row: compute_iv(row, r), axis=1)\n",
    "    chain_snapshot[\"delta\"] = chain_snapshot.apply(lambda row: compute_delta(row, r), axis=1)\n",
    "    \n",
    "    # 8. Calculate exit dates (using config setting for trading vs calendar days)\n",
    "    chain_snapshot['date_exit'] = calculate_exit_dte_dates(\n",
    "        chain_snapshot['expiration'], \n",
    "        exit_dte,\n",
    "        use_trading_days=use_trading_days_exit\n",
    "    )\n",
    "    \n",
    "    # 9. Add entry date\n",
    "    chain_snapshot['date'] = chain_snapshot['ts_event'].dt.date\n",
    "    \n",
    "    # 10. Filter candidates\n",
    "    candidates = chain_snapshot[\n",
    "        (chain_snapshot[\"call_put\"] == config['option_type'])\n",
    "        & chain_snapshot[\"dte\"].between(config['min_dte'], config['max_dte'])\n",
    "        & chain_snapshot[\"delta\"].abs().between(config['min_delta'], config['max_delta'])\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\"\\nFound {len(candidates)} candidates passing filters\")\n",
    "    print(f\"  DTE calculation: {'trading days' if use_trading_days_filter else 'calendar days'}\")\n",
    "    print(f\"  Exit date calculation: {'trading days' if use_trading_days_exit else 'calendar days'}\")\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 11. Filter out same-day duplicates (symbols already held from this entry date for this ticker)\n",
    "    if len(positions_df) > 0:\n",
    "        # Only filter if we have positions from the SAME entry date AND ticker\n",
    "        same_date_positions = positions_df[\n",
    "            (positions_df['entry_date'] == entry_date) & \n",
    "            (positions_df['ticker'] == ticker)\n",
    "        ]\n",
    "        if len(same_date_positions) > 0:\n",
    "            held_symbols = same_date_positions['symbol'].tolist()\n",
    "            candidates = candidates[~candidates['symbol'].isin(held_symbols)]\n",
    "            print(f\"After removing same-day duplicates: {len(candidates)} candidates\")\n",
    "    \n",
    "    if len(candidates) == 0:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # 12. Create backtest candidates\n",
    "    backtest_candidates = candidates.copy()\n",
    "    backtest_candidates['cost_basis'] = backtest_candidates['underlying_last'] * 100 - backtest_candidates['mid']\n",
    "    backtest_candidates['premium'] = backtest_candidates['mid']\n",
    "    backtest_candidates['exit_target_price'] = CONFIG['profit_target_pct'] * backtest_candidates['premium']\n",
    "    backtest_candidates = backtest_candidates[[\n",
    "        'symbol', 'date_exit', 'cost_basis', 'premium', 'exit_target_price',\n",
    "        'date', 'dte', 'expiration', 'mid', 'strike'\n",
    "    ]]\n",
    "    \n",
    "    print(f\"\\nBacktest candidates:\")\n",
    "    print(backtest_candidates[['symbol', 'strike', 'dte', 'premium', 'date_exit']].to_string())\n",
    "    \n",
    "    # 13. Run exit strategy\n",
    "    exits_df = backtest_exit_strategy(backtest_candidates, ticker, client, config)\n",
    "    \n",
    "    # 14. Create new positions (entry info for tracking)\n",
    "    new_positions = backtest_candidates.copy()\n",
    "    new_positions['entry_date'] = entry_date\n",
    "    new_positions['ticker'] = ticker\n",
    "    \n",
    "    return new_positions, exits_df\n",
    "\n",
    "\n",
    "print(\"Process entry date function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Main Backtest Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "main-loop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# Processing ticker: TSLA\n",
      "######################################################################\n",
      "\n",
      "============================================================\n",
      "Processing TSLA on 2023-06-01\n",
      "============================================================\n",
      "  [CACHE HIT] Loading options_TSLA_2023-06-01\n",
      "  [CACHE HIT] Loading equity_TSLA_2023-06-01\n",
      "\n",
      "Found 2 candidates passing filters\n",
      "  DTE calculation: trading days\n",
      "  Exit date calculation: trading days\n",
      "\n",
      "Backtest candidates:\n",
      "                                                    symbol  strike  dte  premium  date_exit\n",
      "ts_recv                                                                                    \n",
      "2023-06-01 15:45:59.509547226-04:00  TSLA  230721P00195000   195.0   33    10.50 2023-06-22\n",
      "2023-06-01 15:45:59.669283153-04:00  TSLA  230721P00190000   190.0   33     8.75 2023-06-22\n",
      "\n",
      "Processing TSLA  230721P00195000...\n",
      "  Entry: 2023-06-01, Premium: $10.50\n",
      "  Exit target: $5.25 (50.0%)\n",
      "  Exit date (21 DTE): 2023-06-22\n",
      "  [CACHE HIT] Loading daily_TSLA__230721P00195000_2023-06-02 00:00:00_2023-06-22 00:00:00\n",
      "  Profit target hit on 2023-06-06 @ $5.25\n",
      "    (Daily range: $4.63 - $5.40)\n",
      "\n",
      "Processing TSLA  230721P00190000...\n",
      "  Entry: 2023-06-01, Premium: $8.75\n",
      "  Exit target: $4.38 (50.0%)\n",
      "  Exit date (21 DTE): 2023-06-22\n",
      "  [CACHE HIT] Loading daily_TSLA__230721P00190000_2023-06-02 00:00:00_2023-06-22 00:00:00\n",
      "  Profit target hit on 2023-06-06 @ $4.38\n",
      "    (Daily range: $3.70 - $4.45)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'exit_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/nissan_options/wheel_strategy/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'exit_price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  SMA entry: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtech_details[\u001b[33m'\u001b[39m\u001b[33msma_entry\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, BB entry: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtech_details[\u001b[33m'\u001b[39m\u001b[33mbb_entry\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Process this ticker/date combination\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m new_positions, exits = \u001b[43mprocess_entry_date\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mentry_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mentry_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpositions_df\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpositions_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCONFIG\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Add new positions\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_positions) > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mprocess_entry_date\u001b[39m\u001b[34m(entry_date, ticker, positions_df, config)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28mprint\u001b[39m(backtest_candidates[[\u001b[33m'\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstrike\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdte\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpremium\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdate_exit\u001b[39m\u001b[33m'\u001b[39m]].to_string())\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# 13. Run exit strategy\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m exits_df = \u001b[43mbacktest_exit_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbacktest_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# 14. Create new positions (entry info for tracking)\u001b[39;00m\n\u001b[32m    107\u001b[39m new_positions = backtest_candidates.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mbacktest_exit_strategy\u001b[39m\u001b[34m(backtest_candidates, ticker, client, config)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# Calculate P&L\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exits_df) > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     exits_df[\u001b[33m'\u001b[39m\u001b[33mexit_pnl\u001b[39m\u001b[33m'\u001b[39m] = exits_df[\u001b[33m'\u001b[39m\u001b[33mpremium\u001b[39m\u001b[33m'\u001b[39m] - \u001b[43mexits_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexit_price\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    111\u001b[39m     exits_df[\u001b[33m'\u001b[39m\u001b[33mexit_pnl_pct\u001b[39m\u001b[33m'\u001b[39m] = (exits_df[\u001b[33m'\u001b[39m\u001b[33mexit_pnl\u001b[39m\u001b[33m'\u001b[39m] / exits_df[\u001b[33m'\u001b[39m\u001b[33mpremium\u001b[39m\u001b[33m'\u001b[39m]) * \u001b[32m100\u001b[39m\n\u001b[32m    112\u001b[39m     exits_df[\u001b[33m'\u001b[39m\u001b[33mroc\u001b[39m\u001b[33m'\u001b[39m] = (exits_df[\u001b[33m'\u001b[39m\u001b[33mexit_pnl\u001b[39m\u001b[33m'\u001b[39m] / exits_df[\u001b[33m'\u001b[39m\u001b[33mcost_basis\u001b[39m\u001b[33m'\u001b[39m]) * \u001b[32m100\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/nissan_options/wheel_strategy/.venv/lib/python3.14/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/nissan_options/wheel_strategy/.venv/lib/python3.14/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'exit_price'"
     ]
    }
   ],
   "source": [
    "# Initialize tracking DataFrames\n",
    "positions_df = pd.DataFrame()  # Active positions\n",
    "all_exits_df = pd.DataFrame()  # All completed trades\n",
    "skipped_entries = []  # Ticker/date combinations that failed technical filter\n",
    "\n",
    "# Process each ticker and entry date combination\n",
    "for ticker in CONFIG['tickers']:\n",
    "    print(f\"\\n{'#'*70}\")\n",
    "    print(f\"# Processing ticker: {ticker}\")\n",
    "    print('#'*70)\n",
    "\n",
    "    for entry_date in CONFIG['entry_dates']:\n",
    "\n",
    "        # Check technical filter first (per ticker)\n",
    "        if CONFIG.get('technical_filter_enabled', False):\n",
    "            passes_filter, tech_details = check_technical_entry(\n",
    "                ticker, entry_date, CONFIG\n",
    "            )\n",
    "\n",
    "            if not passes_filter:\n",
    "                print(f\"\\n[SKIPPED] {ticker} on {entry_date} - Failed technical filter\")\n",
    "                if 'close' in tech_details:\n",
    "                    print(f\"  Close: ${tech_details['close']:.2f}, SMA: ${tech_details['sma']:.2f}, BB Lower: ${tech_details['bb_lower']:.2f}\")\n",
    "                skipped_entries.append({'ticker': ticker, 'date': entry_date, 'reason': 'technical_filter', **tech_details})\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"\\n[PASSED] {ticker} on {entry_date} - Technical filter passed\")\n",
    "                if 'close' in tech_details:\n",
    "                    print(f\"  Close: ${tech_details['close']:.2f}, SMA: ${tech_details['sma']:.2f}\")\n",
    "                    print(f\"  SMA entry: {tech_details['sma_entry']}, BB entry: {tech_details['bb_entry']}\")\n",
    "\n",
    "        # Process this ticker/date combination\n",
    "        new_positions, exits = process_entry_date(\n",
    "            entry_date=entry_date,\n",
    "            ticker=ticker,\n",
    "            positions_df=positions_df,\n",
    "            config=CONFIG\n",
    "        )\n",
    "\n",
    "        # Add new positions\n",
    "        if len(new_positions) > 0:\n",
    "            positions_df = pd.concat([positions_df, new_positions], ignore_index=True)\n",
    "\n",
    "        # Accumulate exits\n",
    "        if len(exits) > 0:\n",
    "            all_exits_df = pd.concat([all_exits_df, exits], ignore_index=True)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"BACKTEST COMPLETE\")\n",
    "print('='*70)\n",
    "print(f\"Tickers processed: {CONFIG['tickers']}\")\n",
    "total_combinations = len(CONFIG['tickers']) * len(CONFIG['entry_dates'])\n",
    "print(f\"Total ticker/date combinations: {total_combinations}\")\n",
    "if CONFIG.get('technical_filter_enabled', False):\n",
    "    print(f\"Combinations passing technical filter: {total_combinations - len(skipped_entries)}\")\n",
    "    print(f\"Combinations skipped (failed filter): {len(skipped_entries)}\")\n",
    "    if len(skipped_entries) > 0:\n",
    "        skipped_df = pd.DataFrame(skipped_entries)\n",
    "        print(f\"\\nSkipped by ticker:\")\n",
    "        print(skipped_df['ticker'].value_counts().to_string())\n",
    "print(f\"\\nTotal positions entered: {len(positions_df)}\")\n",
    "print(f\"Total exits: {len(all_exits_df)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. Results - Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_exits_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"AGGREGATE RESULTS (All Tickers)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nExit reasons:\")\n",
    "    print(all_exits_df['exit_reason'].value_counts())\n",
    "    \n",
    "    print(\"\\nP&L Summary:\")\n",
    "    print(all_exits_df[['exit_pnl', 'exit_pnl_pct', 'roc']].describe())\n",
    "    \n",
    "    print(f\"\\nTotal P&L: ${all_exits_df['exit_pnl'].sum():.2f}\")\n",
    "    print(f\"Average ROC: {all_exits_df['roc'].mean():.2f}%\")\n",
    "    print(f\"Win Rate: {(all_exits_df['exit_pnl'] > 0).mean() * 100:.1f}%\")\n",
    "    \n",
    "    print(\"\\nAll exits:\")\n",
    "    display_cols = ['ticker', 'symbol', 'entry_date', 'exit_date', 'premium', 'exit_price', \n",
    "                   'exit_pnl', 'roc', 'exit_reason']\n",
    "    print(all_exits_df[display_cols].to_string())\n",
    "else:\n",
    "    print(\"No exits recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. Results - By Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "results-by-ticker",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_exits_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS BY TICKER\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for ticker in CONFIG['tickers']:\n",
    "        ticker_exits = all_exits_df[all_exits_df['ticker'] == ticker]\n",
    "        \n",
    "        if len(ticker_exits) == 0:\n",
    "            print(f\"\\n{ticker}: No exits\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{'-'*40}\")\n",
    "        print(f\"{ticker}\")\n",
    "        print(f\"{'-'*40}\")\n",
    "        \n",
    "        print(f\"  Trades: {len(ticker_exits)}\")\n",
    "        print(f\"  Total P&L: ${ticker_exits['exit_pnl'].sum():.2f}\")\n",
    "        print(f\"  Avg P&L: ${ticker_exits['exit_pnl'].mean():.2f}\")\n",
    "        print(f\"  Avg ROC: {ticker_exits['roc'].mean():.2f}%\")\n",
    "        print(f\"  Win Rate: {(ticker_exits['exit_pnl'] > 0).mean() * 100:.1f}%\")\n",
    "        print(f\"  Avg Days Held: {ticker_exits['days_held'].mean():.1f}\")\n",
    "        \n",
    "        print(f\"\\n  Exit reasons:\")\n",
    "        for reason, count in ticker_exits['exit_reason'].value_counts().items():\n",
    "            print(f\"    {reason}: {count}\")\n",
    "else:\n",
    "    print(\"No exits recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "## 11. Ticker Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ticker-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(all_exits_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TICKER COMPARISON\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison = all_exits_df.groupby('ticker').agg({\n",
    "        'exit_pnl': ['count', 'sum', 'mean'],\n",
    "        'roc': 'mean',\n",
    "        'days_held': 'mean',\n",
    "    }).round(2)\n",
    "    \n",
    "    comparison.columns = ['Trades', 'Total P&L', 'Avg P&L', 'Avg ROC %', 'Avg Days']\n",
    "    \n",
    "    # Add win rate\n",
    "    win_rates = all_exits_df.groupby('ticker').apply(\n",
    "        lambda x: (x['exit_pnl'] > 0).mean() * 100\n",
    "    ).round(1)\n",
    "    comparison['Win Rate %'] = win_rates\n",
    "    \n",
    "    print(comparison.to_string())\n",
    "else:\n",
    "    print(\"No exits recorded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "## 12. Validation\n",
    "\n",
    "To validate, set:\n",
    "- `tickers = ['TSLA']`\n",
    "- `entry_dates = ['2023-06-06']`\n",
    "\n",
    "Expected TSLA results:\n",
    "\n",
    "| Symbol | Entry Date | Exit Date | Exit Reason | Premium | Exit Price |\n",
    "|--------|------------|-----------|-------------|---------|------------|\n",
    "| TSLA 230721P00200000 | 2023-06-06 | 2023-06-08 | profit_target | ~$7.85 | ~$3.92 |\n",
    "| TSLA 230721P00205000 | 2023-06-06 | 2023-06-08 | profit_target | ~$9.53 | ~$4.76 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation check for TSLA\n",
    "expected_tsla_symbols = ['TSLA  230721P00200000', 'TSLA  230721P00205000']\n",
    "\n",
    "if len(all_exits_df) > 0:\n",
    "    print(\"Validation check (TSLA):\")\n",
    "    for sym in expected_tsla_symbols:\n",
    "        match = all_exits_df[all_exits_df['symbol'] == sym]\n",
    "        if len(match) > 0:\n",
    "            row = match.iloc[0]\n",
    "            print(f\"  {sym}:\")\n",
    "            print(f\"    Exit date: {row['exit_date']}\")\n",
    "            print(f\"    Exit reason: {row['exit_reason']}\")\n",
    "            print(f\"    Premium: ${row['premium']:.2f}\")\n",
    "            print(f\"    Exit price: ${row['exit_price']:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {sym}: NOT FOUND (may not be in tickers list)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
