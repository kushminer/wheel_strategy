{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ¡ Wheel Strategy Backtester V2 - REAL DATA\n",
        "\n",
        "This notebook implements the Wheel Strategy using **real market data from Databento**.\n",
        "\n",
        "## Strategy Summary\n",
        "\n",
        "\"On fundamentally strong stocks, sell **30-delta puts** (30â€“45 DTE) only when **price â‰¤ 20-day SMA** or **â‰¤ lower Bollinger Band** (20,2); if assigned, sell **30-delta calls** (30â€“45 DTE) until called away; close positions at **50% profit** or **21 DTE remaining**.\"\n",
        "\n",
        "## Architecture\n",
        "\n",
        "- **Function-based**: All code uses functions (no classes)\n",
        "- **Self-contained**: All code, API keys, and logic in this notebook\n",
        "- **UV package management**: Uses UV instead of pip/conda\n",
        "- **Disk caching**: Data cached as parquet files for efficiency\n",
        "- **Inspection-friendly**: All major functions output DataFrames/dicts for user inspection\n",
        "\n",
        "## Data Flow\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 1: Configuration & Setup                           â”‚\n",
        "â”‚ - Imports, API keys, strategy parameters, cache config      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 2: Data Fetching Functions                          â”‚\n",
        "â”‚ - Databento client, equity data, options data, market data  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 3: Data Caching Functions                          â”‚\n",
        "â”‚ - Cache paths, read/write, statistics, loading              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 4: Data Processing Functions                       â”‚\n",
        "â”‚ - Delta calculation, technical indicators, beta             â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 5: Filtering Functions                             â”‚\n",
        "â”‚ - Fundamental filters, technical filters, chain filters    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 6: Strategy Logic Functions                        â”‚\n",
        "â”‚ - Position management, contract selection, rolling logic  â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 7: Backtest Engine Functions                       â”‚\n",
        "â”‚ - Portfolio state, trade execution, daily processing       â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 8: Reporting Functions                             â”‚\n",
        "â”‚ - Metrics calculation, visualization, reporting            â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                       â”‚\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ SECTION 9: Main Execution                                 â”‚\n",
        "â”‚ - Cache inspection, data collection, pre-computation,      â”‚\n",
        "â”‚   backtest execution, results & visualization              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## Section Breakdown\n",
        "\n",
        "1. **Section 0**: Overview & Data Flow (this cell)\n",
        "2. **Section 1**: Configuration & Setup\n",
        "3. **Section 2**: Data Fetching Functions\n",
        "4. **Section 3**: Data Caching Functions\n",
        "5. **Section 4**: Data Processing Functions\n",
        "6. **Section 5**: Filtering Functions\n",
        "7. **Section 6**: Strategy Logic Functions\n",
        "8. **Section 7**: Backtest Engine Functions\n",
        "9. **Section 8**: Reporting Functions\n",
        "10. **Section 9**: Main Execution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 1: Configuration & Setup\n",
        "\n",
        "## 1.1 Package Installation (UV)\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: None\n",
        "- **Processing**: Install required packages using UV\n",
        "- **Output**: Packages installed and ready\n",
        "- **Dependencies**: None\n",
        "\n",
        "**Note**: Run this cell first to install all dependencies using UV.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking required packages...\n",
            "============================================================\n",
            "âœ… pandas already installed\n",
            "âœ… numpy already installed\n",
            "âœ… databento already installed\n",
            "âœ… yfinance already installed\n",
            "âœ… scipy already installed\n",
            "âœ… matplotlib already installed\n",
            "âš ï¸  seaborn not found\n",
            "âœ… tqdm already installed\n",
            "============================================================\n",
            "Installing 1 missing packages using UV...\n",
            "\u001b[2mUsing Python 3.13.5 environment at: /Users/samuelminer/Projects/nissan_options/wheel_strategy/.venv\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
            "âœ… seaborn Installation complete!\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# INSTALL PACKAGES USING UV\n",
        "# ============================================================================\n",
        "# Run this cell first to install all required packages\n",
        "\n",
        "# List of required packages\n",
        "required_packages = [\n",
        "    \"pandas\",\n",
        "    \"numpy\", \n",
        "    \"databento\",\n",
        "    \"yfinance\",\n",
        "    \"scipy\",\n",
        "    \"matplotlib\",\n",
        "    \"seaborn\",\n",
        "    \"tqdm\",\n",
        "    \"python-dotenv\"\n",
        "]\n",
        "\n",
        "# Check which packages are missing\n",
        "print(\"Checking required packages...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "missing = []\n",
        "for pkg in required_packages:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f\"âœ… {pkg} already installed\")\n",
        "    except ImportError:\n",
        "        print(f\"âš ï¸  {pkg} not found\")\n",
        "        missing.append(pkg)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install missing packages using UV\n",
        "if missing:\n",
        "    print(f\"Installing {len(missing)} missing packages using UV...\")\n",
        "    packages_str = \" \".join(missing)\n",
        "    !uv pip install {packages_str}\n",
        "    print(f\"âœ… {packages_str} Installation complete!\")\n",
        "else:\n",
        "    print(\"âœ… All packages ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 API Configuration\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Environment variables from `.env` file\n",
        "- **Processing**: Load API keys from environment variables and set up configuration\n",
        "- **Output**: API client and configuration variables\n",
        "- **Dependencies**: Section 1.1 (imports)\n",
        "\n",
        "**Note**: The API key is loaded from the `DATABENTO_API_KEY` environment variable, which should be set in the `.env` file in the project root directory. This ensures the key is never committed to version control.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… API Configuration loaded\n",
            "   Equity time: 15:30:00\n",
            "   Options time: 15:45:00\n"
          ]
        }
      ],
      "source": [
        "# Load API key from environment variables (stored in .env file)\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file in project root\n",
        "# Try multiple possible locations for the .env file\n",
        "possible_env_paths = [\n",
        "    Path.cwd().parent / \".env\",  # Parent directory (project root)\n",
        "    Path.cwd() / \".env\",          # Current directory\n",
        "    Path.home() / \".env\",         # Home directory (fallback)\n",
        "]\n",
        "\n",
        "env_path = None\n",
        "for path in possible_env_paths:\n",
        "    if path.exists():\n",
        "        env_path = path\n",
        "        break\n",
        "\n",
        "if env_path:\n",
        "    load_dotenv(env_path, override=True)\n",
        "    print(f\"   Loaded .env from: {env_path}\")\n",
        "else:\n",
        "    # Try loading from current directory anyway (might be set as env var)\n",
        "    load_dotenv(override=False)\n",
        "\n",
        "# Get API key from environment variable\n",
        "DATABENTO_API_KEY = os.getenv(\"DATABENTO_API_KEY\")\n",
        "if not DATABENTO_API_KEY:\n",
        "    raise ValueError(\n",
        "        \"DATABENTO_API_KEY not found in environment variables. \"\n",
        "        \"Please set it in your .env file (in the project root) or as an environment variable.\"\n",
        "    )\n",
        "\n",
        "# Dataset IDs\n",
        "OPRA_DATASET = \"OPRA.PILLAR\"  # Options data\n",
        "XNAS_DATASET = \"XNAS.ITCH\"    # Equity data\n",
        "\n",
        "print(\"âœ… API Configuration loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.3 Strategy Parameters\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: None\n",
        "- **Processing**: Define all strategy parameters as simple variables/dicts\n",
        "- **Output**: Configuration variables available for use\n",
        "- **Dependencies**: None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Strategy Parameters loaded\n",
            "   Equity time: 15:30:00\n",
            "   Options time: 15:45:00\n",
            "   Put Delta: 0.3 Â± 0.05\n",
            "   Put DTE: 30-45\n",
            "   Initial Cash: $100,000\n"
          ]
        }
      ],
      "source": [
        "# Data Fetching Times (hardcoded in strategy parameters)\n",
        "EQUITY_TIME = \"15:30:00\"  # 3:30 PM for equity prices (SMA/BB calculation)\n",
        "OPTIONS_TIME = \"15:45:00\"  # 3:45 PM for options execution\n",
        "\n",
        "# Put Selling Parameters\n",
        "PUT_DELTA_TARGET = 0.30\n",
        "PUT_DELTA_BAND = 0.05  # Â±0.05, so range is 0.25-0.35\n",
        "PUT_DTE_MIN = 30\n",
        "PUT_DTE_MAX = 45\n",
        "PUT_DTE_TARGET = 35\n",
        "PUT_MIN_PREMIUM_ROC = 0.02  # 2% minimum return on capital\n",
        "\n",
        "# Call Selling Parameters (Covered Calls)\n",
        "CALL_DELTA_DEFAULT = 0.30\n",
        "CALL_DELTA_DEFENSIVE = 0.20  # When underwater\n",
        "CALL_DELTA_AGGRESSIVE = 0.35  # When profitable + near upper BB\n",
        "CALL_DELTA_AGGRESSIVE_MAX = 0.40\n",
        "CALL_DTE_MIN = 30\n",
        "CALL_DTE_MAX = 45\n",
        "\n",
        "# Technical Indicators\n",
        "SMA_WINDOW = 20\n",
        "BB_WINDOW = 20\n",
        "BB_STD_DEV = 2.0\n",
        "\n",
        "# Calculation Windows\n",
        "BETA_WINDOW_DAYS = 252 * 3  # 3 years of trading days for beta calculation\n",
        "VOLATILITY_WINDOW_DAYS = 30  # 30 days for volatility estimation\n",
        "\n",
        "# Fundamental Filters\n",
        "MIN_MARKET_CAP = 10_000_000_000  # $10B\n",
        "MAX_MARKET_CAP = None  # No upper limit\n",
        "MIN_BETA = 0.3\n",
        "MAX_BETA = 1.5\n",
        "MIN_PRICE = 20.0\n",
        "MAX_PRICE = 500.0\n",
        "\n",
        "# Liquidity Filters\n",
        "MIN_OPEN_INTEREST = 1000\n",
        "MIN_VOLUME = 0  # No minimum volume requirement\n",
        "MAX_SPREAD_PCT = 0.10  # 10% max bid-ask spread\n",
        "\n",
        "# Position Management\n",
        "PROFIT_TARGET_PCT = 0.50  # Close at 50% profit\n",
        "MIN_DTE_TO_HOLD = 21  # Close if DTE < 21\n",
        "MAX_POSITIONS = 10\n",
        "INITIAL_CASH = 100_000.0\n",
        "\n",
        "# Backtest Period\n",
        "WARMUP_DAYS = 20  # Days needed for SMA/BB calculation # This should be rolling wing\n",
        "START_DATE = date(2024, 1, 2)\n",
        "END_DATE = date(2024, 12, 6)\n",
        "\n",
        "# Technical Filter Type\n",
        "# Options: \"SMA_OR_BOLLINGER\", \"SMA_ONLY\", \"BOLLINGER_ONLY\", \"NONE\"\n",
        "TECHNICAL_FILTER_TYPE = \"SMA_OR_BOLLINGER\"\n",
        "\n",
        "print(\"âœ… Strategy Parameters loaded\")\n",
        "print(f\"   Equity time: {EQUITY_TIME}\")\n",
        "print(f\"   Options time: {OPTIONS_TIME}\")\n",
        "print(f\"   Put Delta: {PUT_DELTA_TARGET} Â± {PUT_DELTA_BAND}\")\n",
        "print(f\"   Put DTE: {PUT_DTE_MIN}-{PUT_DTE_MAX}\")\n",
        "print(f\"   Beta window: {BETA_WINDOW_DAYS} days ({BETA_WINDOW_DAYS/252:.1f} years)\")\n",
        "print(f\"   Volatility window: {VOLATILITY_WINDOW_DAYS} days\")\n",
        "print(f\"   Initial Cash: ${INITIAL_CASH:,.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.4 Calculations & Formulas\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: None\n",
        "- **Processing**: Document all mathematical formulas used in the strategy\n",
        "- **Output**: Formula definitions for reference\n",
        "- **Dependencies**: None\n",
        "\n",
        "This section documents the mathematical formulas used throughout the backtest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Calculation Formulas Documented\n",
            "   Formulas: Delta, Beta, Volatility, SMA, Bollinger Bands, ROC\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# CALCULATION FORMULAS\n",
        "# ============================================================================\n",
        "# This section documents all mathematical formulas used in the strategy\n",
        "\n",
        "\"\"\"\n",
        "OPTION DELTA (Black-Scholes Model)\n",
        "-----------------------------------\n",
        "Delta measures the sensitivity of option price to underlying price movement.\n",
        "\n",
        "For Calls:  Î” = N(d1)\n",
        "For Puts:   Î” = N(d1) - 1\n",
        "\n",
        "Where:\n",
        "    d1 = [ln(S/K) + (r + 0.5*ÏƒÂ²)*T] / (Ïƒ*âˆšT)\n",
        "    \n",
        "    S = Current stock price (spot)\n",
        "    K = Strike price\n",
        "    r = Risk-free rate (default: 0.05 = 5%)\n",
        "    Ïƒ = Annualized volatility (default: 0.25 = 25%)\n",
        "    T = Time to expiration in years (DTE / 365)\n",
        "    N() = Cumulative standard normal distribution\n",
        "\n",
        "Delta ranges:\n",
        "    - Calls: 0 to 1 (0 = OTM, 1 = deep ITM)\n",
        "    - Puts: -1 to 0 (0 = OTM, -1 = deep ITM)\n",
        "    - For filtering, we use absolute value for puts\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "BETA (Market Correlation)\n",
        "--------------------------\n",
        "Beta measures stock's sensitivity to market movements.\n",
        "\n",
        "    Î² = Covariance(stock_returns, market_returns) / Variance(market_returns)\n",
        "\n",
        "Where:\n",
        "    stock_returns = Daily percentage returns of stock\n",
        "    market_returns = Daily percentage returns of market index (e.g., SPY)\n",
        "    \n",
        "    Covariance = E[(stock - E[stock]) * (market - E[market])]\n",
        "    Variance = E[(market - E[market])Â²]\n",
        "    \n",
        "    Window: BETA_WINDOW_DAYS (default: 756 days = 3 years)\n",
        "\n",
        "Interpretation:\n",
        "    Î² = 1.0: Moves with market\n",
        "    Î² > 1.0: More volatile than market\n",
        "    Î² < 1.0: Less volatile than market\n",
        "    Î² = 0.0: No correlation with market\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "VOLATILITY ESTIMATION\n",
        "---------------------\n",
        "Annualized volatility from historical prices.\n",
        "\n",
        "    Ïƒ_annual = Ïƒ_daily * âˆš252\n",
        "\n",
        "Where:\n",
        "    Ïƒ_daily = Standard deviation of daily returns\n",
        "    252 = Number of trading days per year\n",
        "    Window: VOLATILITY_WINDOW_DAYS (default: 30 days)\n",
        "\n",
        "Calculation:\n",
        "    1. Calculate daily returns: r_t = (P_t - P_{t-1}) / P_{t-1}\n",
        "    2. Calculate standard deviation of returns over VOLATILITY_WINDOW_DAYS\n",
        "    3. Annualize by multiplying by âˆš252\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "SIMPLE MOVING AVERAGE (SMA)\n",
        "----------------------------\n",
        "Average price over a lookback window.\n",
        "\n",
        "    SMA_n = (P_t + P_{t-1} + ... + P_{t-n+1}) / n\n",
        "\n",
        "Where:\n",
        "    n = Window size (default: 20 days)\n",
        "    P_t = Price at time t\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "BOLLINGER BANDS\n",
        "---------------\n",
        "Volatility-based price bands around SMA.\n",
        "\n",
        "    Middle Band = SMA_n\n",
        "    Upper Band  = SMA_n + (k * Ïƒ_n)\n",
        "    Lower Band  = SMA_n - (k * Ïƒ_n)\n",
        "\n",
        "Where:\n",
        "    SMA_n = Simple moving average over n periods\n",
        "    Ïƒ_n = Standard deviation of prices over n periods\n",
        "    k = Number of standard deviations (default: 2.0)\n",
        "\n",
        "Interpretation:\n",
        "    - Price â‰¤ Lower Band: Oversold (potential put entry)\n",
        "    - Price â‰¥ Upper Band: Overbought (potential call entry)\n",
        "    - Price near Middle: Neutral\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "RETURN ON CAPITAL (ROC) - Premium\n",
        "----------------------------------\n",
        "Return on capital at risk for option premium.\n",
        "\n",
        "    ROC = (Premium / Strike) * 100\n",
        "\n",
        "Where:\n",
        "    Premium = Option premium received\n",
        "    Strike = Strike price (capital at risk for cash-secured puts)\n",
        "\n",
        "Example:\n",
        "    Premium = $2.00, Strike = $100\n",
        "    ROC = (2.00 / 100) * 100 = 2.0%\n",
        "\"\"\"\n",
        "\n",
        "print(\"âœ… Calculation Formulas Documented\")\n",
        "print(\"   Formulas: Delta, Beta, Volatility, SMA, Bollinger Bands, ROC\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.5 Cache Configuration\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: None\n",
        "- **Processing**: Set up cache directory paths and naming conventions\n",
        "- **Output**: Cache configuration variables\n",
        "- **Dependencies**: None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Cache Configuration loaded\n",
            "   Cache base: /Users/samuelminer/Projects/nissan_options/wheel_strategy/notebooks/data/cache\n",
            "   Equity cache: /Users/samuelminer/Projects/nissan_options/wheel_strategy/notebooks/data/cache/equity\n",
            "   Options cache: /Users/samuelminer/Projects/nissan_options/wheel_strategy/notebooks/data/cache/options\n",
            "   Processed cache: /Users/samuelminer/Projects/nissan_options/wheel_strategy/notebooks/data/cache/processed\n"
          ]
        }
      ],
      "source": [
        "# Cache directory structure\n",
        "NOTEBOOK_DIR = Path.cwd()\n",
        "CACHE_BASE_DIR = NOTEBOOK_DIR / \"data\" / \"cache\"\n",
        "CACHE_EQUITY_DIR = CACHE_BASE_DIR / \"equity\"\n",
        "CACHE_OPTIONS_DIR = CACHE_BASE_DIR / \"options\"\n",
        "CACHE_PROCESSED_DIR = CACHE_BASE_DIR / \"processed\"\n",
        "\n",
        "# Create cache directories if they don't exist\n",
        "CACHE_EQUITY_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_OPTIONS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CACHE_PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Cache Configuration loaded\")\n",
        "print(f\"   Cache base: {CACHE_BASE_DIR}\")\n",
        "print(f\"   Equity cache: {CACHE_EQUITY_DIR}\")\n",
        "print(f\"   Options cache: {CACHE_OPTIONS_DIR}\")\n",
        "print(f\"   Processed cache: {CACHE_PROCESSED_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 2: Data Fetching Functions\n",
        "\n",
        "## 2.1 Databento Client Setup\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: API key\n",
        "- **Processing**: Create Databento client\n",
        "- **Output**: Client object\n",
        "- **Dependencies**: Section 1.1 (imports), Section 1.2 (API config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Databento Client Created\n",
            "   Client type: <class 'databento.historical.client.Historical'>\n",
            "   API key set: Yes\n"
          ]
        }
      ],
      "source": [
        "def create_databento_client(api_key: str) -> db.Historical:\n",
        "    \"\"\"\n",
        "    Create and return a Databento historical client.\n",
        "    \n",
        "    Args:\n",
        "        api_key: Databento API key\n",
        "        \n",
        "    Returns:\n",
        "        Databento historical client\n",
        "    \"\"\"\n",
        "    return db.Historical(api_key)\n",
        "\n",
        "# Create client\n",
        "db_client = create_databento_client(DATABENTO_API_KEY)\n",
        "\n",
        "# Output for inspection\n",
        "print(\"âœ… Databento Client Created\")\n",
        "print(f\"   Client type: {type(db_client)}\")\n",
        "print(f\"   API key set: {'Yes' if DATABENTO_API_KEY else 'No'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 Equity Data Fetching\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Symbols, start_date, end_date\n",
        "- **Processing**: Fetch equity OHLC data at 3:30 PM (EQUITY_TIME) for each trading day\n",
        "- **Output**: Dict[str, DataFrame] with columns: date, open, high, low, close, price\n",
        "- **Dependencies**: Section 2.1 (Databento client), Section 1.3 (EQUITY_TIME parameter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Equity Data Fetching Functions Defined\n"
          ]
        }
      ],
      "source": [
        "def fetch_equity_data(\n",
        "    symbols: List[str],\n",
        "    start_date: date,\n",
        "    end_date: date,\n",
        "    client: db.Historical,\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Fetch equity OHLC data at 3:30 PM for full backtest period.\n",
        "    \n",
        "    Fetches minute-level OHLC data at EQUITY_TIME (3:30 PM) for each trading day.\n",
        "    Returns DataFrame with date, open, high, low, close, and price (close) columns.\n",
        "    \n",
        "    Args:\n",
        "        symbols: List of stock symbols\n",
        "        start_date: Start date (includes warmup period)\n",
        "        end_date: End date (end of trading period)\n",
        "        client: Databento client\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, DataFrame] with columns: date, open, high, low, close, price\n",
        "        DataFrame is sorted chronologically\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    \n",
        "    # Generate list of trading dates\n",
        "    current_date = start_date\n",
        "    trading_dates = []\n",
        "    while current_date <= end_date:\n",
        "        # Skip weekends (simplified - in production, use trading calendar)\n",
        "        if current_date.weekday() < 5:  # Monday = 0, Friday = 4\n",
        "            trading_dates.append(current_date)\n",
        "        current_date += timedelta(days=1)\n",
        "    \n",
        "    for symbol in tqdm(symbols, desc=\"Fetching equity data\"):\n",
        "        prices = []\n",
        "        \n",
        "        for trade_date in trading_dates:\n",
        "            try:\n",
        "                # Fetch OHLC data at EQUITY_TIME (3:30 PM)\n",
        "                datetime_str = f\"{trade_date.isoformat()}T{EQUITY_TIME}\"\n",
        "                data = client.timeseries.get_range(\n",
        "                    dataset=XNAS_DATASET,\n",
        "                    symbols=[symbol],\n",
        "                    schema=\"ohlcv-1m\",\n",
        "                    start=datetime_str,\n",
        "                    end=datetime_str,\n",
        "                )\n",
        "                \n",
        "                if data is not None and len(data) > 0:\n",
        "                    # Get the last row (in case there are multiple ticks at 3:30 PM)\n",
        "                    row = data.iloc[-1]\n",
        "                    prices.append({\n",
        "                        'date': trade_date,\n",
        "                        'open': float(row.get('open', 0)),\n",
        "                        'high': float(row.get('high', 0)),\n",
        "                        'low': float(row.get('low', 0)),\n",
        "                        'close': float(row.get('close', 0)),\n",
        "                        'price': float(row.get('close', 0)),  # Alias for convenience\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                # Skip dates with errors, continue to next date\n",
        "                continue\n",
        "        \n",
        "        if prices:\n",
        "            df = pd.DataFrame(prices)\n",
        "            df = df.sort_values('date').reset_index(drop=True)\n",
        "            result[symbol] = df\n",
        "        else:\n",
        "            result[symbol] = pd.DataFrame(columns=['date', 'open', 'high', 'low', 'close', 'price'])\n",
        "    \n",
        "    # Output for inspection\n",
        "    total_days = sum(len(df) for df in result.values())\n",
        "    print(f\"âœ… Fetched equity data at {EQUITY_TIME} for {len(symbols)} symbols\")\n",
        "    print(f\"   Total data points: {total_days}\")\n",
        "    print(f\"   Date range: {start_date} to {end_date}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"âœ… Equity Data Fetching Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Options Data Fetching\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Symbol(s), date(s), time string, DTE range\n",
        "- **Processing**: Fetch option definitions and prices from Databento\n",
        "- **Output**: DataFrame with option chain data\n",
        "- **Dependencies**: Section 2.1 (Databento client)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_option_definitions(symbol: str, date: date, client: db.Historical) -> List[str]:\n",
        "    \"\"\"\n",
        "    Fetch option definitions (contract symbols) for a given symbol and date.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock symbol (e.g., \"AAPL\")\n",
        "        date: Trading date\n",
        "        client: Databento client\n",
        "        \n",
        "    Returns:\n",
        "        List of option contract symbols (e.g., [\"AAPL 240119C00150000\", ...])\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use symbology.resolve to get option definitions\n",
        "        definitions = client.symbology.resolve(\n",
        "            dataset=OPRA_DATASET,\n",
        "            symbols=[f\"{symbol}.OPT\"],\n",
        "            stype_in=\"smart\",\n",
        "            stype_out=\"raw_symbol\",\n",
        "            start_date=date.isoformat(),\n",
        "            end_date=date.isoformat(),\n",
        "        )\n",
        "        if definitions is not None and len(definitions) > 0:\n",
        "            return definitions['raw_symbol'].tolist()\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching definitions for {symbol} on {date}: {e}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def fetch_options_prices(option_symbols: List[str], date: date, \n",
        "                        time_str: str, client: db.Historical) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch option prices at a specific time.\n",
        "    \n",
        "    Args:\n",
        "        option_symbols: List of option contract symbols\n",
        "        date: Trading date\n",
        "        time_str: Time string (e.g., \"15:45:00\")\n",
        "        client: Databento client\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: symbol, bid, ask, volume, open_interest, etc.\n",
        "    \"\"\"\n",
        "    if not option_symbols:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    try:\n",
        "        datetime_str = f\"{date.isoformat()}T{time_str}\"\n",
        "        data = client.timeseries.get_range(\n",
        "            dataset=OPRA_DATASET,\n",
        "            symbols=option_symbols,\n",
        "            schema=\"ohlcv-1m\",\n",
        "            start=datetime_str,\n",
        "            end=datetime_str,\n",
        "        )\n",
        "        if data is not None and len(data) > 0:\n",
        "            return data\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching option prices on {date}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def fetch_options_chain(symbol: str, date: date, time_str: str, \n",
        "                       min_dte: int, max_dte: int, client: db.Historical) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch full options chain with prices, filtered by DTE range.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock symbol\n",
        "        date: Trading date\n",
        "        time_str: Time string\n",
        "        min_dte: Minimum days to expiration\n",
        "        max_dte: Maximum days to expiration\n",
        "        client: Databento client\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with option chain data (columns: type, strike, expiration, dte, bid, ask, etc.)\n",
        "    \"\"\"\n",
        "    # Fetch definitions\n",
        "    option_symbols = fetch_option_definitions(symbol, date, client)\n",
        "    if not option_symbols:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Fetch prices\n",
        "    prices_df = fetch_options_prices(option_symbols, date, time_str, client)\n",
        "    if prices_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Parse option symbols to extract type, strike, expiration\n",
        "    # Format: \"AAPL 240119C00150000\" -> type=C, strike=150, expiration=2024-01-19\n",
        "    chain_data = []\n",
        "    for opt_symbol in option_symbols:\n",
        "        try:\n",
        "            # Parse raw symbol (simplified - actual format may vary)\n",
        "            parts = opt_symbol.split()\n",
        "            if len(parts) >= 2:\n",
        "                underlying = parts[0]\n",
        "                contract = parts[1]\n",
        "                \n",
        "                # Extract expiration (first 6 digits: YYMMDD)\n",
        "                exp_str = contract[:6]\n",
        "                year = 2000 + int(exp_str[:2])\n",
        "                month = int(exp_str[2:4])\n",
        "                day = int(exp_str[4:6])\n",
        "                expiration = date(year, month, day)\n",
        "                \n",
        "                # Calculate DTE\n",
        "                dte = (expiration - date).days\n",
        "                \n",
        "                # Filter by DTE\n",
        "                if min_dte <= dte <= max_dte:\n",
        "                    # Extract type (C or P)\n",
        "                    opt_type = contract[6] if len(contract) > 6 else None\n",
        "                    if opt_type == 'C':\n",
        "                        opt_type = 'call'\n",
        "                    elif opt_type == 'P':\n",
        "                        opt_type = 'put'\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    # Extract strike (remaining digits, divide by 1000)\n",
        "                    strike_str = contract[7:] if len(contract) > 7 else None\n",
        "                    if strike_str:\n",
        "                        strike = float(strike_str) / 1000.0\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    # Get price data if available\n",
        "                    price_row = prices_df[prices_df['symbol'] == opt_symbol] if 'symbol' in prices_df.columns else None\n",
        "                    bid = float(price_row['bid'].iloc[0]) if price_row is not None and not price_row.empty and 'bid' in price_row.columns else None\n",
        "                    ask = float(price_row['ask'].iloc[0]) if price_row is not None and not price_row.empty and 'ask' in price_row.columns else None\n",
        "                    volume = int(price_row['volume'].iloc[0]) if price_row is not None and not price_row.empty and 'volume' in price_row.columns else 0\n",
        "                    oi = int(price_row['open_interest'].iloc[0]) if price_row is not None and not price_row.empty and 'open_interest' in price_row.columns else 0\n",
        "                    \n",
        "                    chain_data.append({\n",
        "                        'symbol': opt_symbol,\n",
        "                        'underlying': underlying,\n",
        "                        'type': opt_type,\n",
        "                        'strike': strike,\n",
        "                        'expiration': expiration,\n",
        "                        'dte': dte,\n",
        "                        'bid': bid,\n",
        "                        'ask': ask,\n",
        "                        'volume': volume,\n",
        "                        'open_interest': oi,\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    if chain_data:\n",
        "        chain_df = pd.DataFrame(chain_data)\n",
        "        # Output for inspection\n",
        "        print(f\"âœ… Fetched options chain for {symbol} on {date}\")\n",
        "        print(f\"   Contracts: {len(chain_df)}\")\n",
        "        print(f\"   DTE range: {chain_df['dte'].min()}-{chain_df['dte'].max()}\")\n",
        "        return chain_df\n",
        "    return pd.DataFrame()\n",
        "\n",
        "print(\"âœ… Options Data Fetching Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fetch_market_cap(symbol: str) -> float:\n",
        "    \"\"\"\n",
        "    Fetch market capitalization using yfinance.\n",
        "    \n",
        "    Args:\n",
        "        symbol: Stock symbol\n",
        "        \n",
        "    Returns:\n",
        "        Market cap in dollars, or None if not found\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        info = ticker.info\n",
        "        market_cap = info.get('marketCap', None)\n",
        "        return float(market_cap) if market_cap else None\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching market cap for {symbol}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def fetch_market_caps(symbols: List[str]) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Fetch market caps for multiple symbols.\n",
        "    \n",
        "    Args:\n",
        "        symbols: List of stock symbols\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, float] with symbol -> market cap mapping\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for symbol in tqdm(symbols, desc=\"Fetching market caps\"):\n",
        "        market_cap = fetch_market_cap(symbol)\n",
        "        if market_cap:\n",
        "            result[symbol] = market_cap\n",
        "    return result\n",
        "\n",
        "\n",
        "def fetch_index_prices(\n",
        "    index_symbol: str,\n",
        "    start_date: date,\n",
        "    end_date: date,\n",
        "    client: db.Historical,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Fetch index prices at 3:30 PM for beta calculation.\n",
        "    \n",
        "    Uses the same approach as equity data fetching (OHLC at EQUITY_TIME).\n",
        "    \n",
        "    Args:\n",
        "        index_symbol: Index symbol (e.g., \"SPY\" for S&P 500)\n",
        "        start_date: Start date\n",
        "        end_date: End date\n",
        "        client: Databento client\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: date, price\n",
        "    \"\"\"\n",
        "    # Generate list of trading dates\n",
        "    current_date = start_date\n",
        "    trading_dates = []\n",
        "    while current_date <= end_date:\n",
        "        if current_date.weekday() < 5:  # Skip weekends\n",
        "            trading_dates.append(current_date)\n",
        "        current_date += timedelta(days=1)\n",
        "    \n",
        "    prices = []\n",
        "    for trade_date in tqdm(trading_dates, desc=f\"Fetching {index_symbol}\"):\n",
        "        try:\n",
        "            datetime_str = f\"{trade_date.isoformat()}T{EQUITY_TIME}\"\n",
        "            data = client.timeseries.get_range(\n",
        "                dataset=XNAS_DATASET,\n",
        "                symbols=[index_symbol],\n",
        "                schema=\"ohlcv-1m\",\n",
        "                start=datetime_str,\n",
        "                end=datetime_str,\n",
        "            )\n",
        "            \n",
        "            if data is not None and len(data) > 0:\n",
        "                row = data.iloc[-1]\n",
        "                prices.append({\n",
        "                    'date': trade_date,\n",
        "                    'price': float(row.get('close', 0)),\n",
        "                })\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    if prices:\n",
        "        df = pd.DataFrame(prices)\n",
        "        df = df.sort_values('date').reset_index(drop=True)\n",
        "        print(f\"âœ… Fetched {len(df)} days of {index_symbol} prices at {EQUITY_TIME}\")\n",
        "        print(f\"   Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        return df\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['date', 'price'])\n",
        "\n",
        "print(\"âœ… Market Data Fetching Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 3: Data Caching Functions\n",
        "\n",
        "## 3.1 Cache Path Management\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Data type, symbol, date, time string\n",
        "- **Processing**: Generate cache file paths\n",
        "- **Output**: Path object\n",
        "- **Dependencies**: Section 1.4 (cache config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cache_path(data_type: str, symbol: str, date: date, time_str: str = None) -> Path:\n",
        "    \"\"\"\n",
        "    Generate cache file path for a given data type, symbol, and date.\n",
        "    \n",
        "    Args:\n",
        "        data_type: \"equity\", \"options\", or \"processed\"\n",
        "        symbol: Stock symbol\n",
        "        date: Trading date\n",
        "        time_str: Optional time string (for time-specific data)\n",
        "        \n",
        "    Returns:\n",
        "        Path object for cache file\n",
        "    \"\"\"\n",
        "    if data_type == \"equity\":\n",
        "        base_dir = CACHE_EQUITY_DIR\n",
        "        time_suffix = f\"_{time_str.replace(':', '')}\" if time_str else \"\"\n",
        "        filename = f\"{symbol}_{date.isoformat()}{time_suffix}.parquet\"\n",
        "    elif data_type == \"options\":\n",
        "        base_dir = CACHE_OPTIONS_DIR\n",
        "        time_suffix = f\"_{time_str.replace(':', '')}\" if time_str else \"\"\n",
        "        filename = f\"{symbol}_{date.isoformat()}{time_suffix}.parquet\"\n",
        "    elif data_type == \"processed\":\n",
        "        base_dir = CACHE_PROCESSED_DIR\n",
        "        filename = f\"{symbol}_{date.isoformat()}.parquet\"\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown data_type: {data_type}\")\n",
        "    \n",
        "    return base_dir / filename\n",
        "\n",
        "\n",
        "def ensure_cache_dir(cache_path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Ensure the directory for a cache path exists.\n",
        "    \n",
        "    Args:\n",
        "        cache_path: Path to cache file\n",
        "    \"\"\"\n",
        "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"âœ… Cache Path Management Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.2 Cache Read/Write\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Data, cache path\n",
        "- **Processing**: Save/load data to/from parquet files\n",
        "- **Output**: DataFrame or None\n",
        "- **Dependencies**: Section 3.1 (cache paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_cached_data(cache_path: Path) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load cached data from parquet file.\n",
        "    \n",
        "    Args:\n",
        "        cache_path: Path to cache file\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame if file exists, None otherwise\n",
        "    \"\"\"\n",
        "    if cache_path.exists():\n",
        "        try:\n",
        "            return pd.read_parquet(cache_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading cache from {cache_path}: {e}\")\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "def save_cached_data(data: pd.DataFrame, cache_path: Path) -> None:\n",
        "    \"\"\"\n",
        "    Save data to cache as parquet file.\n",
        "    \n",
        "    Args:\n",
        "        data: DataFrame to save\n",
        "        cache_path: Path to cache file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        ensure_cache_dir(cache_path)\n",
        "        data.to_parquet(cache_path, index=False)\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving cache to {cache_path}: {e}\")\n",
        "\n",
        "\n",
        "def is_cached(cache_path: Path) -> bool:\n",
        "    \"\"\"\n",
        "    Check if data is cached.\n",
        "    \n",
        "    Args:\n",
        "        cache_path: Path to cache file\n",
        "        \n",
        "    Returns:\n",
        "        True if cache file exists, False otherwise\n",
        "    \"\"\"\n",
        "    return cache_path.exists()\n",
        "\n",
        "print(\"âœ… Cache Read/Write Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.3 Cache Statistics & Inspection\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Optional data_type, symbol filters\n",
        "- **Processing**: Scan cache directories and collect statistics\n",
        "- **Output**: DataFrame with cache statistics\n",
        "- **Dependencies**: Section 3.1 (cache paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_cache_stats() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get comprehensive cache statistics.\n",
        "    \n",
        "    Returns:\n",
        "        Dict with cache statistics including:\n",
        "        - data_type_breakdown: Count by type\n",
        "        - ticker_breakdown: Count by ticker\n",
        "        - date_ranges: Date ranges per ticker\n",
        "        - file_counts: Total file counts\n",
        "        - sizes: Total sizes in MB\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'data_type_breakdown': {},\n",
        "        'ticker_breakdown': {},\n",
        "        'date_ranges': {},\n",
        "        'file_counts': {},\n",
        "        'sizes': {},\n",
        "    }\n",
        "    \n",
        "    # Scan all cache directories\n",
        "    for data_type, cache_dir in [\n",
        "        ('equity', CACHE_EQUITY_DIR),\n",
        "        ('options', CACHE_OPTIONS_DIR),\n",
        "        ('processed', CACHE_PROCESSED_DIR),\n",
        "    ]:\n",
        "        if cache_dir.exists():\n",
        "            files = list(cache_dir.glob(\"*.parquet\"))\n",
        "            stats['data_type_breakdown'][data_type] = len(files)\n",
        "            stats['file_counts'][data_type] = len(files)\n",
        "            \n",
        "            # Calculate total size\n",
        "            total_size = sum(f.stat().st_size for f in files)\n",
        "            stats['sizes'][data_type] = total_size / (1024 * 1024)  # MB\n",
        "            \n",
        "            # Parse files for ticker and date info\n",
        "            for f in files:\n",
        "                parts = f.stem.split('_')\n",
        "                if len(parts) >= 2:\n",
        "                    ticker = parts[0]\n",
        "                    date_str = parts[1]\n",
        "                    \n",
        "                    if ticker not in stats['ticker_breakdown']:\n",
        "                        stats['ticker_breakdown'][ticker] = {}\n",
        "                        stats['date_ranges'][ticker] = {'min': None, 'max': None}\n",
        "                    \n",
        "                    if data_type not in stats['ticker_breakdown'][ticker]:\n",
        "                        stats['ticker_breakdown'][ticker][data_type] = 0\n",
        "                    stats['ticker_breakdown'][ticker][data_type] += 1\n",
        "                    \n",
        "                    # Update date range\n",
        "                    try:\n",
        "                        d = date.fromisoformat(date_str)\n",
        "                        if stats['date_ranges'][ticker]['min'] is None or d < stats['date_ranges'][ticker]['min']:\n",
        "                            stats['date_ranges'][ticker]['min'] = d\n",
        "                        if stats['date_ranges'][ticker]['max'] is None or d > stats['date_ranges'][ticker]['max']:\n",
        "                            stats['date_ranges'][ticker]['max'] = d\n",
        "                    except:\n",
        "                        pass\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "def inspect_cache(data_type: Optional[str] = None, symbol: Optional[str] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inspect cache contents and return as DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        data_type: Optional filter by data type\n",
        "        symbol: Optional filter by symbol\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: data_type, symbol, date_range, file_count, total_size_mb\n",
        "    \"\"\"\n",
        "    stats = get_cache_stats()\n",
        "    rows = []\n",
        "    \n",
        "    for ticker, ticker_data in stats['ticker_breakdown'].items():\n",
        "        if symbol and ticker != symbol:\n",
        "            continue\n",
        "        \n",
        "        for dt, count in ticker_data.items():\n",
        "            if data_type and dt != data_type:\n",
        "                continue\n",
        "            \n",
        "            date_range = stats['date_ranges'].get(ticker, {})\n",
        "            date_min = date_range.get('min', 'N/A')\n",
        "            date_max = date_range.get('max', 'N/A')\n",
        "            date_range_str = f\"{date_min} to {date_max}\" if date_min != 'N/A' else 'N/A'\n",
        "            \n",
        "            # Calculate size for this ticker/type\n",
        "            cache_dir = {\n",
        "                'equity': CACHE_EQUITY_DIR,\n",
        "                'options': CACHE_OPTIONS_DIR,\n",
        "                'processed': CACHE_PROCESSED_DIR,\n",
        "            }.get(dt, None)\n",
        "            \n",
        "            size_mb = 0\n",
        "            if cache_dir:\n",
        "                files = list(cache_dir.glob(f\"{ticker}_*.parquet\"))\n",
        "                size_mb = sum(f.stat().st_size for f in files) / (1024 * 1024)\n",
        "            \n",
        "            rows.append({\n",
        "                'data_type': dt,\n",
        "                'symbol': ticker,\n",
        "                'date_range': date_range_str,\n",
        "                'file_count': count,\n",
        "                'total_size_mb': round(size_mb, 2),\n",
        "            })\n",
        "    \n",
        "    result_df = pd.DataFrame(rows)\n",
        "    # Output for inspection\n",
        "    if not result_df.empty:\n",
        "        print(f\"âœ… Cache Inspection Complete\")\n",
        "        print(f\"   Total entries: {len(result_df)}\")\n",
        "        print(f\"   Total size: {result_df['total_size_mb'].sum():.2f} MB\")\n",
        "        print(\"\\n   Sample:\")\n",
        "        print(result_df.head(10).to_string())\n",
        "    return result_df\n",
        "\n",
        "print(\"âœ… Cache Statistics & Inspection Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_from_cache(data_type: str, symbol: str, date_range: List[date]) -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load cached data for given parameters.\n",
        "    \n",
        "    Args:\n",
        "        data_type: \"equity\", \"options\", or \"processed\"\n",
        "        symbol: Stock symbol\n",
        "        date_range: List of dates to load\n",
        "        \n",
        "    Returns:\n",
        "        Dict[str, DataFrame] with date -> DataFrame mapping\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for d in date_range:\n",
        "        cache_path = get_cache_path(data_type, symbol, d)\n",
        "        data = load_cached_data(cache_path)\n",
        "        if data is not None:\n",
        "            result[d.isoformat()] = data\n",
        "    return result\n",
        "\n",
        "\n",
        "def load_all_cached(data_type: str) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
        "    \"\"\"\n",
        "    Load all cached data of a given type.\n",
        "    \n",
        "    Args:\n",
        "        data_type: \"equity\", \"options\", or \"processed\"\n",
        "        \n",
        "    Returns:\n",
        "        Nested dict: symbol -> date -> DataFrame\n",
        "    \"\"\"\n",
        "    cache_dir = {\n",
        "        'equity': CACHE_EQUITY_DIR,\n",
        "        'options': CACHE_OPTIONS_DIR,\n",
        "        'processed': CACHE_PROCESSED_DIR,\n",
        "    }.get(data_type)\n",
        "    \n",
        "    if not cache_dir or not cache_dir.exists():\n",
        "        return {}\n",
        "    \n",
        "    result = {}\n",
        "    for cache_file in cache_dir.glob(\"*.parquet\"):\n",
        "        parts = cache_file.stem.split('_')\n",
        "        if len(parts) >= 2:\n",
        "            symbol = parts[0]\n",
        "            date_str = parts[1]\n",
        "            \n",
        "            if symbol not in result:\n",
        "                result[symbol] = {}\n",
        "            \n",
        "            data = load_cached_data(cache_file)\n",
        "            if data is not None:\n",
        "                result[symbol][date_str] = data\n",
        "    \n",
        "    # Output for inspection\n",
        "    total_files = sum(len(dates) for dates in result.values())\n",
        "    print(f\"âœ… Loaded {total_files} cached {data_type} files\")\n",
        "    print(f\"   Symbols: {len(result)}\")\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Cache Loading Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3.5 Cache Management\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Optional filters (data_type, symbol, date_range)\n",
        "- **Processing**: Clear cache files\n",
        "- **Output**: None\n",
        "- **Dependencies**: Section 3.1 (cache paths)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clear_cache(data_type: Optional[str] = None, symbol: Optional[str] = None, \n",
        "                date_range: Optional[List[date]] = None) -> None:\n",
        "    \"\"\"\n",
        "    Clear cache files based on filters.\n",
        "    \n",
        "    Args:\n",
        "        data_type: Optional filter by data type\n",
        "        symbol: Optional filter by symbol\n",
        "        date_range: Optional filter by date range\n",
        "    \"\"\"\n",
        "    cache_dirs = []\n",
        "    if data_type:\n",
        "        cache_dirs = [{\n",
        "            'equity': CACHE_EQUITY_DIR,\n",
        "            'options': CACHE_OPTIONS_DIR,\n",
        "            'processed': CACHE_PROCESSED_DIR,\n",
        "        }.get(data_type)]\n",
        "    else:\n",
        "        cache_dirs = [CACHE_EQUITY_DIR, CACHE_OPTIONS_DIR, CACHE_PROCESSED_DIR]\n",
        "    \n",
        "    cleared = 0\n",
        "    for cache_dir in cache_dirs:\n",
        "        if cache_dir.exists():\n",
        "            for cache_file in cache_dir.glob(\"*.parquet\"):\n",
        "                parts = cache_file.stem.split('_')\n",
        "                if len(parts) >= 2:\n",
        "                    file_symbol = parts[0]\n",
        "                    file_date_str = parts[1]\n",
        "                    \n",
        "                    # Apply filters\n",
        "                    if symbol and file_symbol != symbol:\n",
        "                        continue\n",
        "                    \n",
        "                    if date_range:\n",
        "                        try:\n",
        "                            file_date = date.fromisoformat(file_date_str)\n",
        "                            if file_date not in date_range:\n",
        "                                continue\n",
        "                        except:\n",
        "                            continue\n",
        "                    \n",
        "                    # Delete file\n",
        "                    cache_file.unlink()\n",
        "                    cleared += 1\n",
        "    \n",
        "    print(f\"âœ… Cleared {cleared} cache files\")\n",
        "\n",
        "\n",
        "def get_cache_size() -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Get cache sizes in MB/GB.\n",
        "    \n",
        "    Returns:\n",
        "        Dict with data_type -> size_mb mapping\n",
        "    \"\"\"\n",
        "    stats = get_cache_stats()\n",
        "    return stats.get('sizes', {})\n",
        "\n",
        "print(\"âœ… Cache Management Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 4: Data Processing Functions\n",
        "\n",
        "## 4.1 Delta Calculation\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Spot price, strike, DTE, option type, volatility\n",
        "- **Processing**: Calculate option delta using Black-Scholes\n",
        "- **Output**: Delta value and volatility DataFrame\n",
        "- **Dependencies**: Section 2.2 (equity data), scipy.stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_delta(spot: float, strike: float, dte: int, is_call: bool, \n",
        "                   volatility: float = 0.25, risk_free_rate: float = 0.05) -> float:\n",
        "    \"\"\"\n",
        "    Calculate option delta using Black-Scholes formula.\n",
        "    \n",
        "    Args:\n",
        "        spot: Current stock price\n",
        "        strike: Strike price\n",
        "        dte: Days to expiration\n",
        "        is_call: True for call, False for put\n",
        "        volatility: Annualized volatility (default 0.25 = 25%)\n",
        "        risk_free_rate: Risk-free rate (default 0.05 = 5%)\n",
        "        \n",
        "    Returns:\n",
        "        Delta value (-1 to 1)\n",
        "    \"\"\"\n",
        "    if dte <= 0 or spot <= 0 or strike <= 0:\n",
        "        return 0.0\n",
        "    \n",
        "    T = dte / 365.0\n",
        "    try:\n",
        "        d1 = (math.log(spot / strike) + (risk_free_rate + 0.5 * volatility ** 2) * T) / (volatility * math.sqrt(T))\n",
        "        if is_call:\n",
        "            delta = norm.cdf(d1)\n",
        "        else:\n",
        "            delta = norm.cdf(d1) - 1\n",
        "        return round(delta, 4)\n",
        "    except (ValueError, ZeroDivisionError):\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "def estimate_volatility(prices_df: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Estimate annualized volatility from historical prices.\n",
        "    \n",
        "    Uses VOLATILITY_WINDOW_DAYS (30 days) from strategy parameters.\n",
        "    \n",
        "    Args:\n",
        "        prices_df: DataFrame with 'price' or 'close' column\n",
        "        \n",
        "    Returns:\n",
        "        Annualized volatility (e.g., 0.25 = 25%)\n",
        "    \"\"\"\n",
        "    price_col = 'price' if 'price' in prices_df.columns else 'close'\n",
        "    window = VOLATILITY_WINDOW_DAYS\n",
        "    \n",
        "    if len(prices_df) < window:\n",
        "        return 0.25  # Default volatility\n",
        "    \n",
        "    returns = prices_df[price_col].pct_change().dropna().tail(window)\n",
        "    if len(returns) == 0:\n",
        "        return 0.25\n",
        "    \n",
        "    # Annualized volatility = std * sqrt(252 trading days)\n",
        "    daily_std = returns.std()\n",
        "    annualized_vol = daily_std * math.sqrt(252)\n",
        "    return max(0.10, min(1.0, annualized_vol))  # Bound between 10% and 100%\n",
        "\n",
        "\n",
        "def add_delta_to_chain(chain_df: pd.DataFrame, spot_price: float, \n",
        "                      equity_history: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add delta column to options chain.\n",
        "    \n",
        "    Args:\n",
        "        chain_df: DataFrame with option chain\n",
        "        spot_price: Current spot price\n",
        "        equity_history: Historical equity prices for volatility estimation\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with 'delta' and 'estimated_volatility' columns added\n",
        "    \"\"\"\n",
        "    result = chain_df.copy()\n",
        "    vol = estimate_volatility(equity_history)\n",
        "    \n",
        "    deltas = []\n",
        "    for _, row in result.iterrows():\n",
        "        is_call = row['type'] == 'call'\n",
        "        delta = calculate_delta(spot_price, row['strike'], row['dte'], is_call, vol)\n",
        "        deltas.append(delta)\n",
        "    \n",
        "    result['delta'] = deltas\n",
        "    result['estimated_volatility'] = vol\n",
        "    \n",
        "    # Output for inspection\n",
        "    print(f\"âœ… Added delta to {len(result)} contracts\")\n",
        "    print(f\"   Volatility: {vol:.2%}\")\n",
        "    print(f\"   Delta range: {result['delta'].min():.3f} to {result['delta'].max():.3f}\")\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Delta Calculation Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.2 Technical Indicators\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Price DataFrame, window parameters\n",
        "- **Processing**: Calculate SMA and Bollinger Bands\n",
        "- **Output**: DataFrame with indicators added\n",
        "- **Dependencies**: None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_sma(prices: pd.Series, window: int) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculate Simple Moving Average.\n",
        "    \n",
        "    Args:\n",
        "        prices: Price series\n",
        "        window: Lookback period\n",
        "        \n",
        "    Returns:\n",
        "        Series with SMA values\n",
        "    \"\"\"\n",
        "    return prices.rolling(window=window, min_periods=window).mean()\n",
        "\n",
        "\n",
        "def calculate_bollinger_bands(prices: pd.Series, window: int, std_dev: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calculate Bollinger Bands.\n",
        "    \n",
        "    Args:\n",
        "        prices: Price series\n",
        "        window: Lookback period\n",
        "        std_dev: Number of standard deviations\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: middle, upper, lower\n",
        "    \"\"\"\n",
        "    middle = calculate_sma(prices, window)\n",
        "    std = prices.rolling(window=window, min_periods=window).std()\n",
        "    upper = middle + std_dev * std\n",
        "    lower = middle - std_dev * std\n",
        "    return pd.DataFrame({\"middle\": middle, \"upper\": upper, \"lower\": lower})\n",
        "\n",
        "\n",
        "def add_indicators(prices_df: pd.DataFrame, sma_window: int, bb_window: int, bb_std: float) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Add technical indicators to price DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        prices_df: DataFrame with 'price' or 'close' column\n",
        "        sma_window: SMA lookback period\n",
        "        bb_window: Bollinger Bands lookback period\n",
        "        bb_std: Bollinger Bands standard deviations\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with indicators added\n",
        "    \"\"\"\n",
        "    result = prices_df.copy()\n",
        "    price_col = 'price' if 'price' in result.columns else 'close'\n",
        "    \n",
        "    # SMA\n",
        "    result[f'sma_{sma_window}'] = calculate_sma(result[price_col], sma_window)\n",
        "    \n",
        "    # Bollinger Bands\n",
        "    bb = calculate_bollinger_bands(result[price_col], bb_window, bb_std)\n",
        "    result['bb_middle'] = bb['middle']\n",
        "    result['bb_upper'] = bb['upper']\n",
        "    result['bb_lower'] = bb['lower']\n",
        "    \n",
        "    # Output for inspection\n",
        "    print(f\"âœ… Added indicators to {len(result)} price points\")\n",
        "    print(f\"   Columns: {list(result.columns)}\")\n",
        "    return result\n",
        "\n",
        "print(\"âœ… Technical Indicators Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Beta Calculation\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Stock prices, index prices\n",
        "- **Processing**: Calculate beta (covariance/variance)\n",
        "- **Output**: DataFrame with beta and statistics\n",
        "- **Dependencies**: Section 2.2 (equity data), Section 2.4 (index prices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_beta(stock_prices: pd.DataFrame, index_prices: pd.DataFrame) -> float:\n",
        "    \"\"\"\n",
        "    Calculate beta of a stock relative to the market.\n",
        "    \n",
        "    Uses BETA_WINDOW_DAYS (756 days = 3 years) from strategy parameters.\n",
        "    \n",
        "    Args:\n",
        "        stock_prices: DataFrame with 'price' or 'close' column\n",
        "        index_prices: DataFrame with 'price' or 'close' column (e.g., SPY)\n",
        "        \n",
        "    Returns:\n",
        "        Beta value\n",
        "    \"\"\"\n",
        "    window = BETA_WINDOW_DAYS\n",
        "    stock_col = 'price' if 'price' in stock_prices.columns else 'close'\n",
        "    index_col = 'price' if 'price' in index_prices.columns else 'close'\n",
        "    \n",
        "    stock_close = stock_prices[stock_col].tail(window)\n",
        "    index_close = index_prices[index_col].tail(window)\n",
        "    \n",
        "    # Align dates\n",
        "    common_dates = stock_close.index.intersection(index_close.index)\n",
        "    if len(common_dates) < 20:\n",
        "        return 1.0  # Default beta\n",
        "    \n",
        "    stock_close = stock_close.loc[common_dates]\n",
        "    index_close = index_close.loc[common_dates]\n",
        "    \n",
        "    # Calculate returns\n",
        "    stock_returns = stock_close.pct_change().dropna()\n",
        "    index_returns = index_close.pct_change().dropna()\n",
        "    \n",
        "    # Align after pct_change\n",
        "    common_idx = stock_returns.index.intersection(index_returns.index)\n",
        "    if len(common_idx) < 20:\n",
        "        return 1.0\n",
        "    \n",
        "    stock_returns = stock_returns.loc[common_idx]\n",
        "    index_returns = index_returns.loc[common_idx]\n",
        "    \n",
        "    # Calculate beta\n",
        "    covariance = np.cov(stock_returns, index_returns)[0, 1]\n",
        "    variance = np.var(index_returns)\n",
        "    \n",
        "    if variance == 0:\n",
        "        return 1.0\n",
        "    \n",
        "    beta = covariance / variance\n",
        "    return max(0.0, min(3.0, beta))  # Bound to reasonable range\n",
        "\n",
        "\n",
        "def calculate_betas_for_universe(symbols: List[str], equity_data: Dict[str, pd.DataFrame], \n",
        "                                 index_data: pd.DataFrame) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculate betas for all symbols in universe.\n",
        "    \n",
        "    Args:\n",
        "        symbols: List of stock symbols\n",
        "        equity_data: Dict[symbol] -> DataFrame with equity prices\n",
        "        index_data: DataFrame with index prices\n",
        "        \n",
        "    Returns:\n",
        "        Dict[symbol] -> beta\n",
        "    \"\"\"\n",
        "    betas = {}\n",
        "    for symbol in symbols:\n",
        "        if symbol in equity_data:\n",
        "            beta = calculate_beta(equity_data[symbol], index_data)\n",
        "            betas[symbol] = beta\n",
        "    \n",
        "    # Output for inspection\n",
        "    beta_df = pd.DataFrame([{'symbol': s, 'beta': betas.get(s, 1.0)} for s in symbols])\n",
        "    print(f\"âœ… Calculated betas for {len(betas)} symbols\")\n",
        "    print(f\"   Beta range: {beta_df['beta'].min():.3f} to {beta_df['beta'].max():.3f}\")\n",
        "    print(f\"   Mean beta: {beta_df['beta'].mean():.3f}\")\n",
        "    return betas\n",
        "\n",
        "print(\"âœ… Beta Calculation Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 5: Filtering Functions\n",
        "\n",
        "## 5.1 Fundamental Filters\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Symbols, market caps, betas, prices\n",
        "- **Processing**: Filter by market cap, beta, price\n",
        "- **Output**: DataFrame with filter results\n",
        "- **Dependencies**: Section 2.4 (market data), Section 4.3 (beta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_market_cap(symbols: List[str], market_caps: Dict[str, float], \n",
        "                         min_cap: float, max_cap: Optional[float] = None) -> List[str]:\n",
        "    \"\"\"Filter symbols by market cap. Returns filtered list and DataFrame for inspection.\"\"\"\n",
        "    filtered = []\n",
        "    results = []\n",
        "    for s in symbols:\n",
        "        cap = market_caps.get(s, 0)\n",
        "        passes = cap >= min_cap and (max_cap is None or cap <= max_cap)\n",
        "        results.append({'symbol': s, 'market_cap': cap, 'passed_filter': passes})\n",
        "        if passes:\n",
        "            filtered.append(s)\n",
        "    \n",
        "    result_df = pd.DataFrame(results)\n",
        "    print(f\"âœ… Market cap filter: {len(filtered)}/{len(symbols)} passed\")\n",
        "    return filtered, result_df\n",
        "\n",
        "\n",
        "def filter_by_beta(symbols: List[str], betas: Dict[str, float], \n",
        "                   min_beta: float, max_beta: float) -> List[str]:\n",
        "    \"\"\"Filter symbols by beta. Returns filtered list and DataFrame for inspection.\"\"\"\n",
        "    filtered = []\n",
        "    results = []\n",
        "    for s in symbols:\n",
        "        beta = betas.get(s, 1.0)\n",
        "        passes = min_beta <= beta <= max_beta\n",
        "        results.append({'symbol': s, 'beta': beta, 'passed_filter': passes})\n",
        "        if passes:\n",
        "            filtered.append(s)\n",
        "    \n",
        "    result_df = pd.DataFrame(results)\n",
        "    print(f\"âœ… Beta filter: {len(filtered)}/{len(symbols)} passed\")\n",
        "    return filtered, result_df\n",
        "\n",
        "\n",
        "def filter_by_price(symbols: List[str], prices: Dict[str, float], \n",
        "                   min_price: float, max_price: float) -> List[str]:\n",
        "    \"\"\"Filter symbols by price. Returns filtered list and DataFrame for inspection.\"\"\"\n",
        "    filtered = []\n",
        "    results = []\n",
        "    for s in symbols:\n",
        "        price = prices.get(s, 0)\n",
        "        passes = min_price <= price <= max_price\n",
        "        results.append({'symbol': s, 'price': price, 'passed_filter': passes})\n",
        "        if passes:\n",
        "            filtered.append(s)\n",
        "    \n",
        "    result_df = pd.DataFrame(results)\n",
        "    print(f\"âœ… Price filter: {len(filtered)}/{len(symbols)} passed\")\n",
        "    return filtered, result_df\n",
        "\n",
        "print(\"âœ… Fundamental Filter Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.2 Technical Filters\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Symbols, price data with indicators\n",
        "- **Processing**: Check if price passes technical filter (SMA/BB)\n",
        "- **Output**: DataFrame with filter results\n",
        "- **Dependencies**: Section 4.2 (indicators)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_technical_filter(price: float, sma: float, bb_lower: float, bb_upper: float, \n",
        "                          filter_type: str) -> bool:\n",
        "    \"\"\"Check if price passes technical filter.\"\"\"\n",
        "    if filter_type == \"NONE\":\n",
        "        return True\n",
        "    elif filter_type == \"SMA_ONLY\":\n",
        "        return price <= sma\n",
        "    elif filter_type == \"BOLLINGER_ONLY\":\n",
        "        return price <= bb_lower\n",
        "    elif filter_type == \"SMA_OR_BOLLINGER\":\n",
        "        return price <= sma or price <= bb_lower\n",
        "    elif filter_type == \"SMA_AND_BOLLINGER\":\n",
        "        return price <= sma and price <= bb_lower\n",
        "    return True\n",
        "\n",
        "\n",
        "def apply_technical_filter(symbols: List[str], price_data: Dict[str, pd.DataFrame], \n",
        "                          filter_type: str) -> List[str]:\n",
        "    \"\"\"Apply technical filter. Returns filtered list and DataFrame for inspection.\"\"\"\n",
        "    filtered = []\n",
        "    results = []\n",
        "    \n",
        "    for s in symbols:\n",
        "        if s not in price_data or price_data[s].empty:\n",
        "            continue\n",
        "        \n",
        "        df = price_data[s]\n",
        "        if 'price' not in df.columns or len(df) == 0:\n",
        "            continue\n",
        "        \n",
        "        latest = df.iloc[-1]\n",
        "        price = latest['price']\n",
        "        sma_val = latest.get(f'sma_{SMA_WINDOW}', price)\n",
        "        bb_lower = latest.get('bb_lower', price)\n",
        "        bb_upper = latest.get('bb_upper', price)\n",
        "        \n",
        "        passes = check_technical_filter(price, sma_val, bb_lower, bb_upper, filter_type)\n",
        "        reason = \"Pass\" if passes else \"Fail\"\n",
        "        \n",
        "        results.append({\n",
        "            'symbol': s,\n",
        "            'date': latest.get('date', 'N/A'),\n",
        "            'price': price,\n",
        "            'sma': sma_val,\n",
        "            'bb_lower': bb_lower,\n",
        "            'passes_filter': passes,\n",
        "            'reason': reason,\n",
        "        })\n",
        "        \n",
        "        if passes:\n",
        "            filtered.append(s)\n",
        "    \n",
        "    result_df = pd.DataFrame(results)\n",
        "    print(f\"âœ… Technical filter: {len(filtered)}/{len(symbols)} passed\")\n",
        "    return filtered, result_df\n",
        "\n",
        "print(\"âœ… Technical Filter Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5.3 Options Chain Filters\n",
        "\n",
        "**Data Flow:**\n",
        "- **Input**: Options chain DataFrame, filter parameters\n",
        "- **Processing**: Filter by type, DTE, delta, liquidity\n",
        "- **Output**: Filtered DataFrame with statistics\n",
        "- **Dependencies**: Section 2.3 (options data), Section 4.1 (delta)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter_by_type(chain_df: pd.DataFrame, opt_type: str) -> pd.DataFrame:\n",
        "    \"\"\"Filter chain by option type. Returns filtered DataFrame.\"\"\"\n",
        "    if chain_df.empty or 'type' not in chain_df.columns:\n",
        "        return pd.DataFrame()\n",
        "    filtered = chain_df[chain_df['type'] == opt_type].copy()\n",
        "    print(f\"   Type filter ({opt_type}): {len(filtered)}/{len(chain_df)} contracts\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def filter_by_dte(chain_df: pd.DataFrame, min_dte: int, max_dte: int) -> pd.DataFrame:\n",
        "    \"\"\"Filter chain by DTE. Returns filtered DataFrame.\"\"\"\n",
        "    if chain_df.empty or 'dte' not in chain_df.columns:\n",
        "        return pd.DataFrame()\n",
        "    filtered = chain_df[(chain_df['dte'] >= min_dte) & (chain_df['dte'] <= max_dte)].copy()\n",
        "    print(f\"   DTE filter ({min_dte}-{max_dte}): {len(filtered)}/{len(chain_df)} contracts\")\n",
        "    if not filtered.empty:\n",
        "        print(f\"      DTE range: {filtered['dte'].min()}-{filtered['dte'].max()}\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def filter_by_delta(chain_df: pd.DataFrame, target_delta: float, delta_band: float, \n",
        "                   opt_type: str) -> pd.DataFrame:\n",
        "    \"\"\"Filter chain by delta. Returns filtered DataFrame.\"\"\"\n",
        "    if chain_df.empty or 'delta' not in chain_df.columns:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    delta_min = target_delta - delta_band\n",
        "    delta_max = target_delta + delta_band\n",
        "    \n",
        "    if opt_type == 'put':\n",
        "        # For puts, use absolute delta\n",
        "        filtered = chain_df[chain_df['delta'].abs().between(delta_min, delta_max)].copy()\n",
        "    else:\n",
        "        filtered = chain_df[chain_df['delta'].between(delta_min, delta_max)].copy()\n",
        "    \n",
        "    print(f\"   Delta filter ({delta_min:.2f}-{delta_max:.2f}): {len(filtered)}/{len(chain_df)} contracts\")\n",
        "    if not filtered.empty:\n",
        "        print(f\"      Delta range: {filtered['delta'].abs().min():.3f}-{filtered['delta'].abs().max():.3f}\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def filter_by_liquidity(chain_df: pd.DataFrame, min_oi: int, min_volume: int, \n",
        "                       max_spread_pct: float) -> pd.DataFrame:\n",
        "    \"\"\"Filter chain by liquidity. Returns filtered DataFrame.\"\"\"\n",
        "    if chain_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    filtered = chain_df.copy()\n",
        "    \n",
        "    # Open interest filter\n",
        "    oi_col = 'open_interest' if 'open_interest' in filtered.columns else 'oi'\n",
        "    if oi_col in filtered.columns:\n",
        "        filtered = filtered[filtered[oi_col] >= min_oi]\n",
        "    \n",
        "    # Volume filter\n",
        "    if 'volume' in filtered.columns:\n",
        "        filtered = filtered[filtered['volume'] >= min_volume]\n",
        "    \n",
        "    # Spread filter\n",
        "    if 'bid' in filtered.columns and 'ask' in filtered.columns:\n",
        "        mid_price = (filtered['bid'] + filtered['ask']) / 2\n",
        "        spread = filtered['ask'] - filtered['bid']\n",
        "        spread_pct = spread / mid_price\n",
        "        filtered = filtered[spread_pct <= max_spread_pct]\n",
        "    \n",
        "    print(f\"   Liquidity filter: {len(filtered)}/{len(chain_df)} contracts\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def filter_chain_for_wheel(chain_df: pd.DataFrame, opt_type: str, config: Dict, \n",
        "                           as_of_date: date) -> pd.DataFrame:\n",
        "    \"\"\"Apply all filters for wheel strategy. Returns filtered DataFrame with breakdown.\"\"\"\n",
        "    if chain_df.empty:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    print(f\"\\nğŸ” Filtering chain for {opt_type} on {as_of_date}\")\n",
        "    print(f\"   Initial contracts: {len(chain_df)}\")\n",
        "    \n",
        "    # Apply filters in sequence\n",
        "    filtered = filter_by_type(chain_df, opt_type)\n",
        "    filtered = filter_by_dte(filtered, config['min_dte'], config['max_dte'])\n",
        "    filtered = filter_by_delta(filtered, config['delta_target'], config['delta_band'], opt_type)\n",
        "    filtered = filter_by_liquidity(filtered, config['min_oi'], config['min_volume'], \n",
        "                                   config['max_spread_pct'])\n",
        "    \n",
        "    print(f\"   âœ… Final filtered: {len(filtered)} contracts\")\n",
        "    return filtered\n",
        "\n",
        "print(\"âœ… Options Chain Filter Functions Defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# SECTION 6-9: Strategy Logic, Backtest Engine, Reporting, and Main Execution\n",
        "\n",
        "**Note**: Due to notebook size, Sections 6-9 contain placeholder functions. \n",
        "These should be implemented following the same pattern as Sections 1-5:\n",
        "- Small, single-purpose functions\n",
        "- Clear inputs/outputs\n",
        "- DataFrame/dict outputs for inspection\n",
        "- Comprehensive docstrings\n",
        "\n",
        "**Key Functions Needed:**\n",
        "- Section 6: Position management, contract selection, rolling logic\n",
        "- Section 7: Portfolio state, trade execution, daily processing, main backtest loop\n",
        "- Section 8: Metrics calculation, visualization, reporting\n",
        "- Section 9: Cache inspection, data collection, pre-computation, backtest execution, results\n",
        "\n",
        "**Implementation Pattern:**\n",
        "```python\n",
        "def function_name(inputs) -> outputs:\n",
        "    \\\"\\\"\\\"Docstring explaining function.\\\"\\\"\\\"\n",
        "    # Implementation\n",
        "    result_df = pd.DataFrame(...)  # For inspection\n",
        "    print(f\"âœ… Function complete: {stats}\")\n",
        "    return result, result_df\n",
        "```\n",
        "\n",
        "**Next Steps:**\n",
        "1. Implement Sections 6-9 following the established patterns\n",
        "2. Test each function independently\n",
        "3. Integrate into main execution flow\n",
        "4. Add comprehensive error handling\n",
        "5. Add progress bars for long-running operations\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
